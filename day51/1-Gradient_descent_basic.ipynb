{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238d697e-9812-48bf-84b6-b0d737f9cb22",
   "metadata": {},
   "source": [
    "### what is Gradient descent ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "637ae245-87f1-4288-88a3-4d506e492f36",
   "metadata": {},
   "source": [
    "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913a13a-2d04-4b18-a57f-4d20a9d210e0",
   "metadata": {},
   "source": [
    "Gradient Descent is used in:\n",
    " - Linear Regression\n",
    " - Logistic Regression\n",
    " - Neural Networks\n",
    " - Deep Learning\n",
    " - Transformers\n",
    " - LLMs\n",
    " - Basically everything that learns weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed8b8d-5d67-471d-a2f1-14cca2f398ee",
   "metadata": {},
   "source": [
    "#### why we need gradient descent ? \n",
    "1. Matrix inversion is computationally expensive\n",
    "2. High memory requirements\n",
    "3. Non-invertible matrix issue\n",
    "4. Better scalability for large datasets\n",
    "5. Generalization to other machine learning models\n",
    "\n",
    "[day50](https://github.com/DiwanshuG/MachineLearningJourney/blob/main/day50/02_Multiple_Linear_Regression_Mathematical_Derivation.ipynb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bef9eb07-fe51-46ed-a02f-24e904802adb",
   "metadata": {},
   "source": [
    "types of gradient descent \n",
    "1) Batch GD\n",
    "2) SGD\n",
    "3) MBGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0212a-c011-4e84-b27c-b80ea0394cbc",
   "metadata": {},
   "source": [
    "### Intuition of algo : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c7d4e-6fde-4efc-96b7-2451fffbdb61",
   "metadata": {},
   "source": [
    "note : Gradient Descent (Through Linear Regression Lens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45242516-0017-4e41-bdcc-389290a763ca",
   "metadata": {},
   "source": [
    "example : a dataset has 2 cols and 4 rows , we have to draw a regression line[best fit] without using OLS method "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebb12796-7d6d-4e32-abda-4b18e0fac690",
   "metadata": {},
   "source": [
    "We aim to find the best-fit line such that the sum of the squared differences between the actual values (y) and the predicted values (ŷ) is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5f627-8eae-40ec-9066-4f7137edda3e",
   "metadata": {},
   "source": [
    "- J = loss function\n",
    "$$\n",
    "J = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a91de-7aca-444a-b16a-990d13124532",
   "metadata": {},
   "source": [
    "##### Given:\n",
    "\n",
    "- \\( n = 4 \\)\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = m x_i + b\n",
    "$$\n",
    "\n",
    "##### Cost Function:\n",
    "\n",
    "$$\n",
    "J(m, b) = \\sum_{i=1}^{4} \\left( y_i - (m x_i + b) \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d696af78-2ff2-402a-829f-54186cce46d3",
   "metadata": {},
   "source": [
    "so loss function depends on m & b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4088c-572d-43c2-a703-28b65a103b6f",
   "metadata": {},
   "source": [
    "##### Assumption:\n",
    "\n",
    "For now, assume we already know the correct value of the slope \\( m \\).\n",
    "\n",
    "Using Ordinary Least Squares (OLS), we found:\n",
    "\n",
    "$$\n",
    "m = 78.35\n",
    "$$\n",
    "\n",
    "Now, we will focus on optimizing only \\( b \\) while keeping \\( m \\) fixed.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58a069e3-5510-41ab-843c-f469ed64c6b3",
   "metadata": {},
   "source": [
    "so , J(b)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "914ca347-9515-48a0-8218-23376c459c63",
   "metadata": {},
   "source": [
    "we have to find the value of b for which the loss function is minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e763857-bb19-498e-96fb-ca2f299e1cb2",
   "metadata": {},
   "source": [
    "##### Objective:\n",
    "\n",
    "We need to find the value of \\( b \\) for which the loss function \\( J(m, b) \\) is minimum, \n",
    "while keeping \\( m \\) fixed.\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$\n",
    "\\min_{b} \\; J(m, b)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bcae9-8042-47cb-b335-e179839a5270",
   "metadata": {},
   "source": [
    "When you expand\n",
    "\n",
    "$$\n",
    "\\left( y_i - (m x_i + b) \\right)^2\n",
    "$$\n",
    "\n",
    "First rewrite it as:\n",
    "\n",
    "$$\n",
    "\\left( y_i - m x_i - b \\right)^2\n",
    "$$\n",
    "\n",
    "Now apply the identity:\n",
    "\n",
    "$$\n",
    "(a - b)^2 = a^2 - 2ab + b^2\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\left( y_i - m x_i - b \\right)^2\n",
    "=\n",
    "(y_i - m x_i)^2\n",
    "- 2b (y_i - m x_i)\n",
    "+ b^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5dc1c-528f-44cb-ae4d-4a42afcee4fe",
   "metadata": {},
   "source": [
    "Since \\( m \\) is fixed, the cost function \\( J(m, b) \\) becomes a function of only \\( b \\).\n",
    "\n",
    "After expansion, it takes the form of a quadratic equation:\n",
    "\n",
    "$$\n",
    "J(b) = A b^2 + B b + C\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "J(b) \\propto b^2\n",
    "$$\n",
    "\n",
    "This means the cost function is a quadratic (parabola) in terms of \\( b \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b25e0c-51b8-458f-b5a3-cf8d80c8c7e8",
   "metadata": {},
   "source": [
    "#### Step 1 : \n",
    "select a random b"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21246c7c-b10c-41b9-a3ad-315537bb1b9c",
   "metadata": {},
   "source": [
    "let , b= -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fac872-761e-4222-95c5-ee42d86a6cc7",
   "metadata": {},
   "source": [
    "As humans, we can look at the graph of the cost function and visually identify \n",
    "the direction in which the minimum lies.\n",
    "\n",
    "But an algorithm cannot \"see\" the graph.\n",
    "\n",
    "It does not know whether it should increase or decrease the value of \\( b \\).\n",
    "\n",
    "So instead of visual intuition, it uses the gradient (derivative) \n",
    "to mathematically determine the direction of steepest increase of the loss function.\n",
    "\n",
    "If the derivative is positive, the function is increasing → decrease \\( b \\).\n",
    "\n",
    "If the derivative is negative, the function is decreasing → increase \\( b \\).\n",
    "\n",
    "Therefore, the algorithm updates the parameter in the opposite direction of the gradient \n",
    "to move toward the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cb20c-f55c-4540-82c9-5f4b953d3493",
   "metadata": {},
   "source": [
    "##### So, we compute the slope (derivative) at the current point and update the parameter accordingly.\n",
    "\n",
    "At any given value of \\( b \\), we calculate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "- If the slope is positive → decrease \\( b \\)\n",
    "- If the slope is negative → increase \\( b \\)\n",
    "- If the slope is zero → we have reached the minimum\n",
    "\n",
    "So, instead of seeing the graph, the algorithm uses the slope at the current point \n",
    "to decide the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77c4fc-7ce5-4a4e-814c-809374400758",
   "metadata": {},
   "source": [
    "##### Gradient Descent Update Rule:\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241377b5-945d-43ee-a401-433ce2ac0ccb",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$$\n",
    "\\alpha = \\text{learning rate (step size)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\text{slope of the cost function at the current point}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39b337ce-824d-4772-95c4-74a8b06095cf",
   "metadata": {},
   "source": [
    "α∈{0.1,0.01,0.001,0.0001}       :generally"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7481d35-9fee-4303-9bac-b4144132faa2",
   "metadata": {},
   "source": [
    "We multiply by α (learning rate) because the gradient only tells us the direction, not how far to move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bc908-6659-4494-94c1-3cf2836d385b",
   "metadata": {},
   "source": [
    "##### When to Stop Gradient Descent?\n",
    "\n",
    "There are three common stopping criteria:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. When the Gradient Becomes (Almost) Zero\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} \\approx 0\n",
    "$$\n",
    "\n",
    "This means we are very close to the minimum and further updates will not significantly reduce the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When the Change in Loss Becomes Very Small\n",
    "\n",
    "$$\n",
    "|J_{\\text{new}} - J_{\\text{old}}| < \\epsilon\n",
    "$$\n",
    "\n",
    "Where \\( \\epsilon \\) is a very small number.\n",
    "\n",
    "This means the loss is no longer decreasing meaningfully, so the algorithm has converged.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Maximum Number of Iterations Reached\n",
    "\n",
    "Stop after a predefined number of iterations (e.g., 1000 or 10,000).\n",
    "\n",
    "This prevents infinite loops and controls training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087ee58-bf53-4a1e-a0b8-90c4e4c8d278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
