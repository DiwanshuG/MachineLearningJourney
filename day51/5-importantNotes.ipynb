{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714ea597-0417-4aae-840b-37648a5dce87",
   "metadata": {},
   "source": [
    "##### Effect of Learning Rate in Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725b443-d732-4a14-8532-caae54927294",
   "metadata": {},
   "source": [
    "###### 1.Learning Rate Too Small (Very Low η)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8aaed31-da53-4ac5-acda-32179a9d3452",
   "metadata": {},
   "source": [
    "What happens:\n",
    "1) Takes very tiny steps\n",
    "2) Converges very slowly\n",
    "3) Training becomes time-consuming\n",
    "4) May get stuck before reaching optimal minimum (practically)\n",
    "\n",
    "Like walking toward a destination with baby steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e34104-dbf5-4fec-bf73-fc6aa02f8e22",
   "metadata": {},
   "source": [
    "###### 2. Learning Rate Too Large (Very High η)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30adfa78-bf1b-45d4-8796-8f347abad184",
   "metadata": {},
   "source": [
    "What happens:\n",
    "\n",
    "1) Overshoots the minimum\n",
    "2) Loss may oscillate\n",
    "3) Can diverge completely\n",
    "4) Training becomes unstable\n",
    "\n",
    "Like jumping too far and missing the target repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a41cf-f89a-4943-8a16-a32917bfbf7a",
   "metadata": {},
   "source": [
    "###### 3.Optimal Learning Rate (Balanced η)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d990e43-ed30-46c5-9df6-e8ce84a4f02c",
   "metadata": {},
   "source": [
    "What happens:\n",
    "\n",
    "1) Converges efficiently\n",
    "2) Stable training\n",
    "3) Reaches minimum in reasonable time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d9732-3e9b-4cd5-b6cd-233d9c5286a2",
   "metadata": {},
   "source": [
    "| Learning Rate | Behavior       | Result                    |\n",
    "| ------------- | -------------- | ------------------------- |\n",
    "| Too Small     | Slow movement  | Very slow convergence     |\n",
    "| Too Large     | Overshooting   | Divergence / oscillation  |\n",
    "| Optimal       | Balanced steps | Fast & stable convergence |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4f3e9c-35c7-49a8-a5c4-3a9225d8e7f9",
   "metadata": {},
   "source": [
    "## Gradient-Based Optimization Framework\n",
    "\n",
    "For most machine learning algorithms, the essential requirement for optimization is the ability to compute the **gradient (derivative) of the loss function** with respect to the model parameters.\n",
    "\n",
    "Once the loss function is defined, the optimization process follows a structured approach:\n",
    "\n",
    "1. Compute the partial derivatives of the loss function.\n",
    "2. Obtain the slope (gradient) for each parameter.\n",
    "3. Update the parameters using the Gradient Descent update rule.\n",
    "\n",
    "For example, in Simple Linear Regression with parameters  \n",
    "- **m** (slope)  \n",
    "- **b** (intercept)  \n",
    "\n",
    "the update rules are:\n",
    "\n",
    "m = m - η (∂L / ∂m)\n",
    "\n",
    "b = b - η (∂L / ∂b)\n",
    "\n",
    "Where:\n",
    "\n",
    "- **η** is the learning rate  \n",
    "- **L** is the loss function  \n",
    "- **∂L / ∂m** and **∂L / ∂b** are the gradients  \n",
    "\n",
    "By repeatedly applying these update rules, the parameters gradually move toward values that minimize the loss function, resulting in the **best-fit model**.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Principle\n",
    "\n",
    "Define a loss function → Compute its gradient → Apply the update rule → Iterate until convergence.\n",
    "\n",
    "This principle forms the foundation of most gradient-based optimization techniques in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310b2e8-03b0-4214-a6fb-420ac38d9ec2",
   "metadata": {},
   "source": [
    "# Effect of Loss Function on Gradient Descent (GD)\n",
    "\n",
    "The loss function plays a central role in Gradient Descent because it defines:\n",
    "\n",
    "- What the model is trying to minimize  \n",
    "- The shape of the optimization landscape  \n",
    "- The direction and magnitude of parameter updates  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Loss Function Determines the Optimization Surface\n",
    "\n",
    "Gradient Descent minimizes a loss function:\n",
    "\n",
    "θ := θ − η ∇L(θ)\n",
    "\n",
    "The geometry of **L(θ)** determines:\n",
    "\n",
    "- Whether the surface is convex or non-convex  \n",
    "- Whether there is a single global minimum or multiple local minima  \n",
    "- How steep or flat the surface is  \n",
    "\n",
    "Example:\n",
    "- **Mean Squared Error (MSE)** → Smooth convex paraboloid (for linear regression)\n",
    "- **Non-convex losses (deep learning)** → Complex landscape with multiple minima\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Loss Function Controls Gradient Behavior\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "∇L(θ)\n",
    "\n",
    "Different loss functions produce different gradients.\n",
    "\n",
    "### Example:\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "L = (1/n) Σ (y − ŷ)²  \n",
    "\n",
    "Gradient ∝ (y − ŷ)\n",
    "\n",
    "- Penalizes large errors heavily  \n",
    "- Produces smooth gradients  \n",
    "- Sensitive to outliers  \n",
    "\n",
    "---\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "L = (1/n) Σ |y − ŷ|\n",
    "\n",
    "Gradient depends on sign(y − ŷ)\n",
    "\n",
    "- Robust to outliers  \n",
    "- Gradient is not smooth at zero  \n",
    "- Convergence may be slower  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Smoothness Affects Convergence\n",
    "\n",
    "- Smooth and differentiable loss → Stable and predictable convergence  \n",
    "- Non-smooth loss → Gradient may be unstable or undefined at some points  \n",
    "- Flat regions → Slow convergence  \n",
    "- Very steep regions → Risk of divergence with large learning rate  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Convex vs Non-Convex Loss\n",
    "\n",
    "### Convex Loss (e.g., Linear Regression + MSE)\n",
    "\n",
    "- One global minimum  \n",
    "- Guaranteed convergence (with proper learning rate)  \n",
    "\n",
    "### Non-Convex Loss (e.g., Deep Neural Networks)\n",
    "\n",
    "- Multiple local minima  \n",
    "- Saddle points  \n",
    "- Convergence depends on initialization  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Effect on Learning Rate Sensitivity\n",
    "\n",
    "Loss curvature affects step size:\n",
    "\n",
    "- Steep curvature → Small learning rate required  \n",
    "- Flat curvature → Larger learning rate acceptable  \n",
    "- Poor scaling → Slow optimization  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ab57a-8a90-45fe-b274-9c0bb962836f",
   "metadata": {},
   "source": [
    "# Saddle Point in Gradient Descent\n",
    "\n",
    "## 1. Definition\n",
    "\n",
    "A **saddle point** is a point on the loss surface where:\n",
    "\n",
    "- The gradient is zero (∇L = 0)\n",
    "- But it is **not** a minimum or maximum\n",
    "\n",
    "At a saddle point:\n",
    "- The function curves upward in one direction\n",
    "- The function curves downward in another direction\n",
    "\n",
    "So it looks like a saddle used in horse riding.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Intuition\n",
    "\n",
    "For a function L(x, y):\n",
    "\n",
    "If\n",
    "\n",
    "∇L(x, y) = 0\n",
    "\n",
    "and the Hessian matrix has both:\n",
    "- Positive eigenvalues\n",
    "- Negative eigenvalues\n",
    "\n",
    "Then the point is a saddle point.\n",
    "\n",
    "This means curvature is mixed — convex in some directions, concave in others.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example Function\n",
    "\n",
    "A classic example:\n",
    "\n",
    "L(x, y) = x² − y²\n",
    "\n",
    "At (0, 0):\n",
    "\n",
    "- Gradient = 0\n",
    "- Along x-axis → behaves like a minimum\n",
    "- Along y-axis → behaves like a maximum\n",
    "\n",
    "Therefore, (0, 0) is a saddle point.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Saddle Points Matter in Gradient Descent\n",
    "\n",
    "In high-dimensional models (like deep neural networks):\n",
    "\n",
    "- Saddle points are very common\n",
    "- They are more frequent than local minima\n",
    "- Gradient becomes very small near them\n",
    "\n",
    "As a result:\n",
    "\n",
    "- Optimization slows down\n",
    "- Training appears to \"stall\"\n",
    "- GD may take many iterations to escape\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why GD Can Escape Saddle Points\n",
    "\n",
    "Unlike local minima:\n",
    "\n",
    "- Saddle points are unstable\n",
    "- Small noise or momentum helps escape\n",
    "- Random initialization often avoids perfect alignment\n",
    "\n",
    "Modern optimizers like:\n",
    "\n",
    "- Momentum\n",
    "- RMSProp\n",
    "- Adam\n",
    "\n",
    "help escape saddle regions faster.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Insight\n",
    "\n",
    "At a saddle point:\n",
    "\n",
    "Gradient = 0  \n",
    "But it is not an optimal solution.\n",
    "\n",
    "This is why checking only ∇L = 0 is not enough to confirm a minimum.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a8db3-a296-4a3a-b111-9de3db97f5a9",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035326a1-2032-40c2-8c7c-0d43701136c2",
   "metadata": {},
   "source": [
    "# Effect of Data on Gradient Descent\n",
    "\n",
    "Data plays a critical role in how Gradient Descent behaves.  \n",
    "The optimization process is not only controlled by the loss function, but also by the **structure, scale, and distribution of the data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Feature Scaling\n",
    "\n",
    "If features have very different scales:\n",
    "\n",
    "Example:\n",
    "- x₁ ranges from 0 to 1\n",
    "- x₂ ranges from 0 to 10,000\n",
    "\n",
    "Then the loss surface becomes **elongated (elliptical)**.\n",
    "\n",
    "Effect on GD:\n",
    "- Oscillations during updates\n",
    "- Slow convergence\n",
    "- Learning rate becomes hard to tune\n",
    "\n",
    "Solution:\n",
    "- Standardization (mean = 0, std = 1)\n",
    "- Normalization\n",
    "\n",
    "Proper scaling makes contours more circular → faster convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Distribution\n",
    "\n",
    "If data is well distributed:\n",
    "\n",
    "- Loss surface is smooth\n",
    "- Gradient directions are stable\n",
    "- Convergence is predictable\n",
    "\n",
    "If data is skewed or poorly distributed:\n",
    "\n",
    "- Loss surface becomes irregular\n",
    "- Updates may fluctuate\n",
    "- Optimization slows down\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Outliers\n",
    "\n",
    "With loss functions like MSE:\n",
    "\n",
    "L = (1/n) Σ (y − ŷ)²\n",
    "\n",
    "Outliers have large squared errors.\n",
    "\n",
    "Effect:\n",
    "- Large gradients\n",
    "- Parameter updates dominated by few points\n",
    "- Model shifts toward outliers\n",
    "- Possible instability\n",
    "\n",
    "Alternative:\n",
    "- MAE\n",
    "- Huber loss\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Noise in Data\n",
    "\n",
    "High noise increases variance in gradient estimates.\n",
    "\n",
    "Effects:\n",
    "- Slower convergence\n",
    "- Less smooth loss surface\n",
    "- Harder to reach true minimum\n",
    "\n",
    "In stochastic GD:\n",
    "- Noisy data increases gradient variance even more\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Multicollinearity (Highly Correlated Features)\n",
    "\n",
    "If features are highly correlated:\n",
    "\n",
    "- Loss surface becomes narrow and curved\n",
    "- Ill-conditioned Hessian matrix\n",
    "- Slow zig-zag descent\n",
    "\n",
    "Effect:\n",
    "- Requires very small learning rate\n",
    "- Convergence becomes slow\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Dataset Size\n",
    "\n",
    "Small dataset:\n",
    "- Gradient estimates unstable\n",
    "- High variance\n",
    "- Risk of overfitting\n",
    "\n",
    "Large dataset:\n",
    "- Stable gradients (Batch GD)\n",
    "- Slower per iteration computation\n",
    "- More reliable convergence\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Linearity of Data\n",
    "\n",
    "If data follows true linear relationship:\n",
    "- Convex loss surface\n",
    "- Clear global minimum\n",
    "- Fast convergence\n",
    "\n",
    "If relationship is non-linear but model is linear:\n",
    "- Model underfits\n",
    "- Loss surface minimum still exists\n",
    "- But solution is biased\n",
    "\n",
    "---\n",
    "\n",
    "Data affects:\n",
    "\n",
    "- Shape of the loss surface\n",
    "- Stability of gradients\n",
    "- Speed of convergence\n",
    "- Sensitivity to learning rate\n",
    "- Final model quality\n",
    "\n",
    "In short:\n",
    "\n",
    "Good data → smooth optimization  \n",
    "Poorly scaled or noisy data → unstable optimization  \n",
    "\n",
    "Gradient Descent is not just about math —  \n",
    "it is deeply influenced by the geometry created by the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241f8ab-69a9-4dd1-9aef-0462c8a9d4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
