{
 "cells": [
  {
   "cell_type": "raw",
   "id": "67e8b34f-e31c-4ce5-be03-c87ae9565806",
   "metadata": {},
   "source": [
    "cgpa | iq | gender | lpa\n",
    "\n",
    "x1 | x2 | x3 | y\n",
    "\n",
    "here data is in 4-D :\n",
    "if it was in 2D then equation should be y = mx+ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae1f28-1059-4429-a183-e3633c0b696b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_3 + \\beta_3 x_4\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32322738-2820-4302-81c2-fc73e8857be0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = yPredicted \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c810c-74b5-4ac7-8ae7-f7a85f864af5",
   "metadata": {},
   "source": [
    "##### Matrix Representation of Predictions\n",
    "\n",
    "$$\n",
    "\\hat{Y} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\hat{y}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_{100}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & x_{13} \\\\\n",
    "1 & x_{21} & x_{22} & x_{23} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{100,1} & x_{100,2} & x_{100,3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bd764-3a8e-48bd-8fc3-fd2dde7400b7",
   "metadata": {},
   "source": [
    "##### General Matrix Form (n rows, m features)\r\n",
    "\r\n",
    "$$\r\n",
    "\\hat{Y}_{(n \\times 1)} =\r\n",
    "\\begin{bmatrix}\r\n",
    "\\hat{y}_1 \\\\\r\n",
    "\\hat{y}_2 \\\\\r\n",
    "\\vdots \\\\\r\n",
    "\\hat{y}_n\r\n",
    "\\end{bmatrix}\r\n",
    "=\r\n",
    "\\begin{bmatrix}\r\n",
    "1 & x_{11} & x_{12} & \\dots & x_{1m} \\\\\r\n",
    "1 & x_{21} & x_{22} & \\dots & x_{2m} \\\\\r\n",
    "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\r\n",
    "1 & x_{n1} & x_{n2} & \\dots & x_{nm}\r\n",
    "\\end{bmatrix}\r\n",
    "\\begin{bmatrix}\r\n",
    "\\beta_0 \\\\\r\n",
    "\\beta_1 \\\\\r\n",
    "\\beta_2 \\\\\r\n",
    "\\vdots \\\\\r\n",
    "\\beta_m\r\n",
    "\\end{bmatrix}\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e46465-01db-4360-a40d-c712b484554d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{Y} = X\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bb423-8a12-43a7-88cd-d8da4eb9ac6d",
   "metadata": {},
   "source": [
    "##### Error (Residual) Vector : \n",
    "$$\n",
    "e = Y - \\hat{Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b1a10-0236-4e34-a0c3-feb0a97359f1",
   "metadata": {},
   "source": [
    "### Squared Error (Loss)\n",
    "\n",
    "$$\n",
    "E = e^T e                        \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf8305-23cc-4098-8d2b-1380add35a90",
   "metadata": {},
   "source": [
    "where E : \n",
    "$$\n",
    "E = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a7314-20f7-4996-935c-c8453275176d",
   "metadata": {},
   "source": [
    "##### Expansion of Squared Error Term\n",
    "\n",
    "We start with the total squared error:\n",
    "\n",
    "$$\n",
    "E = e^T e = (Y - \\hat{Y})^T (Y - \\hat{Y})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Substitute \\( \\hat{Y} = X\\beta \\)\n",
    "\n",
    "$$\n",
    "E = (Y - X\\beta)^T (Y - X\\beta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Apply transpose property\n",
    "\n",
    "Using:\n",
    "$$\n",
    "(A - B)^T = A^T - B^T\n",
    "$$\n",
    "\n",
    "We get:\n",
    "\n",
    "$$\n",
    "E = (Y^T - (X\\beta)^T)(Y - X\\beta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Expand the multiplication\n",
    "\n",
    "$$\n",
    "E = Y^T Y - Y^T X\\beta - (X\\beta)^T Y + (X\\beta)^T X\\beta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Final expanded form\n",
    "\n",
    "$$\n",
    "E = Y^T Y\n",
    "\\;-\\; Y^T X\\beta\n",
    "\\;-\\; (X\\beta)^T Y\n",
    "\\;+\\; (X\\beta)^T X\\beta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363051f-218b-4cb0-a329-c2b91bf747c1",
   "metadata": {},
   "source": [
    "### Notes !!!\r\n",
    "\r\n",
    "- **E** is a scalar value  \r\n",
    "- **YᵀXβ** and **(Xβ)ᵀY** are scalars and are equal  \r\n",
    "- This expanded error expression is used to:\r\n",
    "  - Take the derivative with respect to **β**\r\n",
    "  - Derive the **Normal Equation**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f918f28-5744-4a9c-8f5a-b9581fb2c871",
   "metadata": {},
   "source": [
    "### Error Function (Quadratic Form)\r\n",
    "\r\n",
    "$$\r\n",
    "E = Y^T Y - 2Y^T X\\beta + \\beta^T X^T X \\beta\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Derivative with respect to \\( \\beta \\)\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\partial E}{\\partial \\beta}\r\n",
    "=\r\n",
    "\\frac{\\partial}{\\partial \\beta}\r\n",
    "\\left[\r\n",
    "Y^T Y - 2Y^T X\\beta + \\beta^T X^T X \\beta\r\n",
    "\\right]\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Derivative of each term\r\n",
    "\r\n",
    "**1. First term**\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\partial}{\\partial \\beta}(Y^T Y) = 0\r\n",
    "$$\r\n",
    "\r\n",
    "(because it does not depend on \\( \\beta \\))\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**2. Second term**\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\partial}{\\partial \\beta}(-2Y^T X\\beta) = -2X^T Y\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**3. Third term**\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\partial}{\\partial \\beta}(\\beta^T X^T X \\beta) = 2X^T X\\beta\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Combine all terms\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\partial E}{\\partial \\beta}\r\n",
    "=\r\n",
    "-2X^T Y + 2X^T X\\beta\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Set derivative to zero (minimization)\r\n",
    "\r\n",
    "$$\r\n",
    "-2X^T Y + 2X^T X\\beta = 0\r\n",
    "$$\r\n",
    "\r\n",
    "Divide both sides by 2:\r\n",
    "\r\n",
    "$$\r\n",
    "X^T X\\beta = X^T Y\r\n",
    "$$\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Normal Equation\r\n",
    "\r\n",
    "$$\r\n",
    "\\boxed{\r\n",
    "\\beta = (X^T X)^{-1} X^T Y\r\n",
    "}\r\n",
    "$$\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f73053c-52bf-451b-b342-a90b818acd8a",
   "metadata": {},
   "source": [
    "X = X_train ,  Y = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ece0d-52f0-470f-b828-4e7d9c945910",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f78727-4e5d-4ab5-a619-fc682d7ab249",
   "metadata": {},
   "source": [
    "##### why Gradient Descent ??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caf5a4c7-227a-43fe-99ac-9796fbf06929",
   "metadata": {},
   "source": [
    "sklearn solved in two ways  : \n",
    "1) OLS [our method here]\n",
    "2) Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d372d2-1b5a-4c4f-830d-530ad08b3375",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boxed{(X^T X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d26cf-2cf4-4813-aced-d53e269d44cd",
   "metadata": {},
   "source": [
    "##### 1. Matrix inversion is computationally expensive\n",
    "The normal equation requires computing the inverse of \\( X^T X \\), which has a time complexity of O(m³) . This becomes infeasible when the number of features is large.\n",
    "\n",
    "##### 2. High memory requirements\n",
    "The normal equation needs the entire dataset and intermediate matrices (\\( X \\), \\( X^T \\), \\( X^T X \\)) to be stored in memory, which is not practical for large datasets.\n",
    "\n",
    "##### 3. Non-invertible matrix issue\n",
    "If features are highly correlated, duplicated, or if the number of features exceeds the number of samples, \\( X^T X \\) may not be invertible. Gradient descent does not suffer from this limitation.\n",
    "\n",
    "##### 4. Better scalability for large datasets\n",
    "Gradient descent works iteratively and supports batch, mini-batch, and stochastic updates, making it suitable for very large datasets.\n",
    "\n",
    "##### 5. Generalization to other machine learning models\n",
    "The normal equation applies only to linear regression, whereas gradient descent is a general optimization technique used in logistic regression, neural networks, and deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7733c-b3fd-4470-8838-5516383c9a9a",
   "metadata": {},
   "source": [
    "| Aspect                 | Normal Equation | Gradient Descent   |\n",
    "| ---------------------- | --------------- | ------------------ |\n",
    "| Time complexity        | O(m³)           | O(mn × iterations) |\n",
    "| Memory usage           | High            | Low                |\n",
    "| Large datasets         | ❌               | ✅                  |\n",
    "| Invertibility required | Yes             | No                 |\n",
    "| General-purpose        | ❌               | ✅                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aab033-96cf-4136-8ed9-44a11ac884fc",
   "metadata": {},
   "source": [
    "[Computational_complexity_of_mathematical_operations(read for matrix inversion)](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994d3db-3f90-4016-9d0b-9eec10ef2d34",
   "metadata": {},
   "source": [
    "### OLS Linear Regression vs SGDRegressor\n",
    "\n",
    "**OLS (Ordinary Least Squares) Linear Regression**\n",
    "- Solves the problem using the **Normal Equation**\n",
    "- Computes an exact solution\n",
    "- Requires matrix inversion \\( (X^T X)^{-1} \\)\n",
    "- Computationally expensive for large feature sets\n",
    "- Used by `LinearRegression()` in sklearn\n",
    "\n",
    "**SGDRegressor**\n",
    "- Uses **Stochastic Gradient Descent**\n",
    "- Finds an approximate solution iteratively\n",
    "- Does not require matrix inversion\n",
    "- Scales well to large datasets\n",
    "- Suitable for online and streaming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc30df-2c3e-49ad-b7b6-4709db9fa42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
