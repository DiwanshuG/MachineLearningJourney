{
 "cells": [
  {
   "cell_type": "raw",
   "id": "71fbef61-d42d-4117-8b38-1ee7d7ab323a",
   "metadata": {},
   "source": [
    "Why does Lasso Regression create sparsity?\n",
    "\n",
    "Why does it make some coefficients exactly zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b25d7e-604c-439d-a82c-26af0eee8de6",
   "metadata": {},
   "source": [
    "#### Why Does Lasso Regression Create Sparsity?\n",
    "\n",
    "Lasso Regression minimizes the following objective function:\n",
    "$$\n",
    "Loss = Σ (y - ŷ)² + λ Σ |wᵢ|\n",
    "$$\n",
    "The key difference from Ridge Regression is the L1 penalty term:\n",
    "$$\n",
    "λ Σ |wᵢ|\n",
    "$$\n",
    "This absolute value term is the reason Lasso produces sparse models.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) Role of the L1 Penalty\n",
    "\n",
    "The L1 penalty adds a cost proportional to the absolute value of the coefficients.\n",
    "\n",
    "Unlike L2 (w²), the absolute value function:\n",
    "\n",
    "- Is not differentiable at zero\n",
    "- Has a sharp \"V\" shape\n",
    "- Applies constant pressure toward zero\n",
    "\n",
    "This makes small coefficients more likely to become exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Gradient Behavior Near Zero\n",
    "\n",
    "For Ridge (L2):\n",
    "$$\n",
    "d/dw (w²) = 2w\n",
    "$$\n",
    "When w is small, the gradient is also small.  \n",
    "So coefficients shrink smoothly but rarely become exactly zero.\n",
    "\n",
    "For Lasso (L1):\n",
    "\n",
    "d/dw (|w|) =\n",
    "+1 , if w > 0  \n",
    "-1  , if w < 0  \n",
    "\n",
    "Near zero, the gradient does not become small — it remains constant.  \n",
    "This strong push drives coefficients directly to zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Geometric Interpretation (Most Important Intuition)\n",
    "\n",
    "Lasso can also be seen as:\n",
    "\n",
    "Minimize RSS subject to ||w||₁ ≤ t\n",
    "\n",
    "The L1 constraint region forms a diamond shape.\n",
    "\n",
    "When RSS contours (ellipses) expand outward, they are more likely to touch the diamond at its sharp corners.\n",
    "\n",
    "Those corners lie on the coordinate axes.\n",
    "\n",
    "When the solution touches a corner:\n",
    "- One or more coefficients become exactly zero.\n",
    "\n",
    "This is sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) Why Ridge Does Not Create Sparsity\n",
    "\n",
    "Ridge uses an L2 constraint:\n",
    "$$\n",
    "||w||₂ ≤ t\n",
    "$$\n",
    "The constraint region is circular (smooth boundary).\n",
    "\n",
    "Since there are no sharp corners:\n",
    "- The solution rarely lies exactly on an axis.\n",
    "- Coefficients shrink but almost never become exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5) Optimization Insight\n",
    "\n",
    "Because |w| is not differentiable at zero, the optimization uses subgradients.\n",
    "\n",
    "At zero, the subgradient contains a range of values.\n",
    "\n",
    "This allows zero to satisfy optimality conditions easily.\n",
    "\n",
    "As a result, many coefficients are set exactly to zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6) Final Conclusion\n",
    "\n",
    "Lasso creates sparsity because:\n",
    "\n",
    "- The L1 penalty has sharp corners.\n",
    "- It applies constant shrinkage pressure.\n",
    "- The optimization process prefers axis-aligned solutions.\n",
    "\n",
    "Therefore, Lasso performs automatic feature selection by forcing some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a2b08-7985-4ca8-9315-97c566b71a37",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression (1 Feature)\n",
    "\n",
    "We assume a linear relationship between X and Y:\n",
    "\n",
    "$$\n",
    "Y = mX + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $m$ = slope  \n",
    "- $b$ = intercept  \n",
    "\n",
    "---\n",
    "\n",
    "#### Intercept Formula\n",
    "\n",
    "The intercept can be written as:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m\\bar{x}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\text{mean}(y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\text{mean}(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Slope Formula (Closed-Form Solution)\n",
    "\n",
    "$$\n",
    "m = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}\n",
    "         {\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "The numerator represents the covariance between $X$ and $Y$.\n",
    "\n",
    "The denominator represents the variance of $X$.\n",
    "\n",
    "So we can also write:\n",
    "\n",
    "$$\n",
    "m = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Regression Equation\n",
    "\n",
    "Once $m$ and $b$ are computed:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = mX + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0173b8-662d-43a6-8747-4a4864a04dab",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b40e4f-ee9c-4e89-8a7d-9b5166a2b90a",
   "metadata": {},
   "source": [
    "#### Ridge Regression (Closed-Form Slope in 1D)\n",
    "\n",
    "In simple linear regression, the slope is:\n",
    "\n",
    "$$\n",
    "m = \n",
    "\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}\n",
    "     {\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Ridge Regression Modification\n",
    "\n",
    "When we add L2 regularization (Ridge), the denominator changes:\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}\n",
    "     {\\sum_{i=1}^{n} (x_i - \\bar{x})^2 + \\lambda}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### What Changed?\n",
    "\n",
    "The only difference is:\n",
    "\n",
    "$$\n",
    "\\sum (x_i - \\bar{x})^2\n",
    "\\quad \\longrightarrow \\quad\n",
    "\\sum (x_i - \\bar{x})^2 + \\lambda\n",
    "$$\n",
    "\n",
    "This additional $\\lambda$ term:\n",
    "\n",
    "- Increases the denominator\n",
    "- Shrinks the slope value\n",
    "- Prevents very large coefficients\n",
    "- Reduces variance\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "As $\\lambda$ increases:\n",
    "\n",
    "$$\n",
    "m \\rightarrow 0\n",
    "$$\n",
    "\n",
    "But it never becomes exactly zero (unless $\\lambda \\to \\infty$).\n",
    "\n",
    "This is why Ridge shrinks coefficients but does not create sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d990f-2b12-439e-8628-174717b4d74b",
   "metadata": {},
   "source": [
    "#### Lasso Regression in 1D (Setup)\n",
    "\n",
    "We start from the simple linear model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = mx + b\n",
    "$$\n",
    "\n",
    "The intercept can be written as:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m\\bar{x}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Lasso Loss Function\n",
    "\n",
    "The Lasso objective function is:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda |m|\n",
    "$$\n",
    "\n",
    "Substituting $\\hat{y}_i = mx_i + b$:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - (mx_i + b))^2 + \\lambda |m|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Substitute Intercept Expression\n",
    "\n",
    "Using:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m\\bar{x}\n",
    "$$\n",
    "\n",
    "We substitute into the loss:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - m x_i - \\bar{y} + m \\bar{x}\n",
    "\\right)^2\n",
    "+ \\lambda |m|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Goal\n",
    "\n",
    "We now want to find:\n",
    "\n",
    "$$\n",
    "m = ?\n",
    "$$\n",
    "\n",
    "That minimizes:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n}\n",
    "(y_i - m x_i - \\bar{y} + m \\bar{x})^2\n",
    "+ \\lambda |m|\n",
    "$$\n",
    "\n",
    "This is where L1 regularization makes the solution different from Ridge.\n",
    "\n",
    "Because of the absolute value term:\n",
    "\n",
    "$$\n",
    "|m|\n",
    "$$\n",
    "\n",
    "The function is not differentiable at $m = 0$.\n",
    "\n",
    "This leads to the soft-thresholding solution and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73dc34a-a671-41f4-9168-884de797e205",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab889136-6f0c-4376-9db9-753b2cb5781f",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84422ec2-3454-4853-a90d-6538c2bd196e",
   "metadata": {},
   "source": [
    "We start from:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\sum_{i=1}^{n}\n",
    "(y_i - m x_i - \\bar{y} + m \\bar{x})^2\n",
    "+ \\lambda |m|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 1: Assume $m > 0$\n",
    "\n",
    "If $m > 0$, then:\n",
    "\n",
    "$$\n",
    "|m| = m\n",
    "$$\n",
    "\n",
    "So the loss becomes:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\sum_{i=1}^{n}\n",
    "(y_i - m x_i - \\bar{y} + m \\bar{x})^2\n",
    "+ \\lambda m\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Simplify the Expression\n",
    "\n",
    "Notice:\n",
    "\n",
    "$$\n",
    "y_i - \\bar{y} - m(x_i - \\bar{x})\n",
    "$$\n",
    "\n",
    "So loss becomes:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "(y_i - \\bar{y}) - m(x_i - \\bar{x})\n",
    "\\right)^2\n",
    "+ \\lambda m\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Differentiate w.r.t. $m$\n",
    "\n",
    "Take derivative:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dm}\n",
    "=\n",
    "-2 \\sum_{i=1}^{n}\n",
    "(x_i - \\bar{x})\n",
    "\\left[\n",
    "(y_i - \\bar{y}) - m(x_i - \\bar{x})\n",
    "\\right]\n",
    "+ \\lambda\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Set Derivative = 0\n",
    "\n",
    "$$\n",
    "-2 \\sum (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "+ 2m \\sum (x_i - \\bar{x})^2\n",
    "+ \\lambda = 0\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "2m \\sum (x_i - \\bar{x})^2\n",
    "=\n",
    "2 \\sum (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "- \\lambda\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Solve for $m$\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\n",
    "\\sum (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "- \\frac{\\lambda}{2}\n",
    "}{\n",
    "\\sum (x_i - \\bar{x})^2\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation (Important)\n",
    "\n",
    "Compare with OLS slope:\n",
    "\n",
    "$$\n",
    "m_{OLS} =\n",
    "\\frac{\n",
    "\\sum (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "}{\n",
    "\\sum (x_i - \\bar{x})^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Lasso subtracts:\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{2}\n",
    "$$\n",
    "\n",
    "from the numerator.\n",
    "\n",
    "This is the shrinkage effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851fbeb-1e69-499d-98e2-dd6f3c3b9076",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e57bbc-5d67-4f97-b814-eb30f52f31b0",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81163ce8-dd9d-42db-aff4-67da9a971c55",
   "metadata": {},
   "source": [
    "##### if We Use 2λ|m| Before Differentiation?\n",
    "\n",
    "The original Lasso loss function is written as:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda |m|\n",
    "$$\n",
    "\n",
    "However, before differentiating, we rewrite it as:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + 2\\lambda |m|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Why Add the Factor 2?\n",
    "\n",
    "When differentiating the squared error term, we get:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dm}(z^2) = 2z \\frac{dz}{dm}\n",
    "$$\n",
    "\n",
    "So the derivative of the RSS part naturally produces a factor of 2.\n",
    "\n",
    "If we keep the original loss:\n",
    "\n",
    "$$\n",
    "L = RSS + \\lambda |m|\n",
    "$$\n",
    "\n",
    "Then after differentiation we get:\n",
    "\n",
    "$$\n",
    "2mS_{xx} - 2S_{xy} + \\lambda = 0\n",
    "$$\n",
    "\n",
    "We would then have to divide everything by 2, which introduces a messy $\\lambda/2$ term.\n",
    "\n",
    "---\n",
    "\n",
    "##### Cleaner Form Using 2λ|m|\n",
    "\n",
    "If we instead define:\n",
    "\n",
    "$$\n",
    "L = RSS + 2\\lambda |m|\n",
    "$$\n",
    "\n",
    "Then after differentiation (for $m > 0$):\n",
    "\n",
    "$$\n",
    "-2S_{xy} + 2mS_{xx} + 2\\lambda = 0\n",
    "$$\n",
    "\n",
    "Dividing by 2 gives:\n",
    "\n",
    "$$\n",
    "mS_{xx} = S_{xy} - \\lambda\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "m = \\frac{S_{xy} - \\lambda}{S_{xx}}\n",
    "$$\n",
    "\n",
    "This form is cleaner and avoids extra fractions.\n",
    "\n",
    "---\n",
    "\n",
    "##### Important Insight\n",
    "\n",
    "Multiplying the penalty term by 2 does NOT change the model conceptually.\n",
    "\n",
    "It simply rescales $\\lambda$.\n",
    "\n",
    "Since $\\lambda$ is a hyperparameter, scaling it does not affect the theory — it only changes its numerical value.\n",
    "\n",
    "This is a common mathematical trick used to simplify differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a78c59-4def-4e7c-9486-f426dec1dd28",
   "metadata": {},
   "source": [
    "##### 1)  Lasso (For $m > 0$)\n",
    "\n",
    "For the case when $m > 0$, the solution becomes:\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x}) - \\lambda\n",
    "}{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f526cb-efa9-4482-87b9-8e3a50d9f1d1",
   "metadata": {},
   "source": [
    "##### 2)  Lasso (For $m = 0$ Case Boundary Condition)\n",
    "\n",
    "At the boundary case, when regularization dominates and the coefficient becomes zero:\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})\n",
    "}{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "}\n",
    "$$\n",
    "\n",
    "This is simply the Ordinary Least Squares (OLS) solution.\n",
    "\n",
    "If the shrinkage term $\\lambda$ is large enough such that:\n",
    "\n",
    "$$\n",
    "\\left| \\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x}) \\right| \\le \\lambda\n",
    "$$\n",
    "\n",
    "then the optimal solution becomes:\n",
    "\n",
    "$$\n",
    "m = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63d764-7d5b-447d-8a0f-2d9811dc1737",
   "metadata": {},
   "source": [
    "##### 3) Lasso (For $m < 0$)\n",
    "\n",
    "For the case when $m < 0$, the solution becomes:\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x}) + \\lambda\n",
    "}{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c042c-2c84-4127-9476-93ee09db39c7",
   "metadata": {},
   "source": [
    "## so why sparsity ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa607ab8-4ee6-40ed-9b58-a2c83e5aec59",
   "metadata": {},
   "source": [
    "##### Why Does Lasso Create Sparsity? (Intuitive Explanation)\n",
    "\n",
    "Let’s understand this in simple words.\n",
    "\n",
    "We derived that for the case when $m > 0$:\n",
    "\n",
    "$$\n",
    "m = \\frac{S_{xy} - \\lambda}{S_{xx}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S_{xy}$ depends only on the data\n",
    "- $\\lambda$ is the regularization strength\n",
    "- $S_{xx} > 0$\n",
    "\n",
    "---\n",
    "\n",
    "##### What Happens When We Increase λ?\n",
    "\n",
    "- $S_{xy}$ is fixed.\n",
    "- As $\\lambda$ increases, the numerator decreases.\n",
    "- So the value of $m$ keeps decreasing.\n",
    "\n",
    "When:\n",
    "\n",
    "$$\n",
    "\\lambda = S_{xy}\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "m = 0\n",
    "$$\n",
    "\n",
    "So the slope becomes exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "##### What Happens If We Increase λ Even More?\n",
    "\n",
    "Mathematically, the formula suggests $m$ would become negative.\n",
    "\n",
    "But here is the important point:\n",
    "\n",
    "That formula was derived assuming $m > 0$.\n",
    "\n",
    "If $m$ tries to become negative, the assumption breaks.\n",
    "\n",
    "So we must switch to the $m < 0$ case.\n",
    "\n",
    "When we check the optimization conditions carefully, we find:\n",
    "\n",
    "Once $m$ reaches zero, it stays at zero.\n",
    "\n",
    "It does NOT cross to the negative side.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why Does It Stay at Zero?\n",
    "\n",
    "Because at $m = 0$, the loss function satisfies the optimality condition.\n",
    "\n",
    "In simple words:\n",
    "\n",
    "- The regularization penalty is strong enough.\n",
    "- The model decides that having this feature is not worth it.\n",
    "- So it removes the feature completely.\n",
    "\n",
    "The coefficient becomes exactly zero and stays there.\n",
    "\n",
    "---\n",
    "\n",
    "##### Intuition in Simple Words\n",
    "\n",
    "Lasso says:\n",
    "\n",
    "\"If this feature is not strong enough to overcome the penalty, remove it.\"\n",
    "\n",
    "As λ increases:\n",
    "\n",
    "- Small coefficients shrink.\n",
    "- When they become small enough, they collapse to zero.\n",
    "- After reaching zero, they do not flip sign.\n",
    "- They remain zero.\n",
    "\n",
    "This is called **sparsity**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Lasso creates sparsity because:\n",
    "\n",
    "- The L1 penalty subtracts a fixed amount (λ) from the slope.\n",
    "- When the slope becomes zero, it gets \"stuck\" there.\n",
    "- Increasing λ further does not revive it.\n",
    "- The feature is effectively removed from the model.\n",
    "\n",
    "That is why Lasso performs automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5f9bd-1957-467a-a099-928118bfc2a1",
   "metadata": {},
   "source": [
    "#### Value of $m$ in Ridge Regression\n",
    "\n",
    "For Ridge Regression, the loss function is:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "+ \\lambda m^2\n",
    "$$\n",
    "\n",
    "After differentiating with respect to $m$ and setting the derivative equal to zero, the slope becomes:\n",
    "\n",
    "$$\n",
    "m =\n",
    "\\frac{\n",
    "\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})\n",
    "}{\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2 + \\lambda\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa40d8-3aa1-4203-ae5c-97693a5fef35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
