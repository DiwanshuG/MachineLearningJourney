{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ff3bcc-ec38-4ed0-b477-d2542437ae77",
   "metadata": {},
   "source": [
    "# Bias–Variance Tradeoff\n",
    "\n",
    "Bias–Variance Tradeoff is a fundamental concept in Machine Learning that explains the balance between underfitting and overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Understanding Prediction Error\n",
    "\n",
    "The expected prediction error of a model can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Bias²** → Error due to simplifying assumptions in the model\n",
    "- **Variance** → Error due to sensitivity to training data\n",
    "- **Irreducible Error** → Noise inherent in the data (cannot be eliminated)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bias\n",
    "\n",
    "### Definition:\n",
    "Bias is the error introduced by approximating a real-world problem with a simplified model.\n",
    "\n",
    "### Characteristics:\n",
    "- Model is too simple\n",
    "- Fails to capture true patterns\n",
    "- High training error\n",
    "- High testing error\n",
    "\n",
    "### Example:\n",
    "Fitting a linear model to quadratic data:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "when the true relationship is:\n",
    "\n",
    "$$\n",
    "y = 0.8x^2 + 0.9x + 2\n",
    "$$\n",
    "\n",
    "This results in **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Variance\n",
    "\n",
    "### Definition:\n",
    "Variance is the error caused by a model being too sensitive to small fluctuations in training data.\n",
    "\n",
    "### Characteristics:\n",
    "- Model is too complex\n",
    "- Fits noise in training data\n",
    "- Very low training error\n",
    "- High testing error\n",
    "\n",
    "### Example:\n",
    "Using a high-degree polynomial (degree = 30) on noisy quadratic data.\n",
    "\n",
    "This results in **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Complexity and Tradeoff\n",
    "\n",
    "As model complexity increases:\n",
    "\n",
    "- Bias decreases\n",
    "- Variance increases\n",
    "\n",
    "As model complexity decreases:\n",
    "\n",
    "- Bias increases\n",
    "- Variance decreases\n",
    "\n",
    "This inverse relationship creates the tradeoff.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visual Interpretation\n",
    "\n",
    "- Underfitting → High Bias, Low Variance\n",
    "- Good Fit → Balanced Bias and Variance\n",
    "- Overfitting → Low Bias, High Variance\n",
    "\n",
    "The goal is to find the optimal complexity where test error is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training vs Testing Error Behavior\n",
    "\n",
    "| Model Complexity | Training Error | Testing Error | Problem        |\n",
    "|------------------|----------------|---------------|---------------|\n",
    "| Very Low         | High           | High          | Underfitting  |\n",
    "| Optimal          | Low            | Low           | Good Fit      |\n",
    "| Very High        | Very Low       | High          | Overfitting   |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Irreducible Error\n",
    "\n",
    "Irreducible error is caused by randomness or noise in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "y = 0.8x^2 + 0.9x + 2 + \\epsilon\n",
    "$$\n",
    "\n",
    "Even a perfect model cannot eliminate \\( \\epsilon \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practical Implications\n",
    "\n",
    "To control bias and variance:\n",
    "\n",
    "- Increase data size → Reduces variance\n",
    "- Reduce model complexity → Reduces variance\n",
    "- Increase model complexity → Reduces bias\n",
    "- Use regularization → Controls variance\n",
    "- Use cross-validation → Select optimal complexity\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Goal of Machine Learning\n",
    "\n",
    "The objective is not to minimize training error,  \n",
    "but to minimize **generalization error**.\n",
    "\n",
    "That means finding the balance where:\n",
    "\n",
    "- Bias is reasonably low\n",
    "- Variance is reasonably low\n",
    "- Test performance is optimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bac6de-bfc7-42b2-b06b-fe55c716b542",
   "metadata": {},
   "source": [
    "| Model   | Training Error | Testing Error | Interpretation |\n",
    "| ------- | -------------- | ------------- | -------------- |\n",
    "| Model A | 8              | 10            | Low variance   |\n",
    "| Model B | 4              | 22            | High variance  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c565ebc-be77-4a01-a22a-d406cd23ebb2",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "Overfitting and underfitting are two common problems in Machine Learning that affect a model’s ability to generalize to new data.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Underfitting\n",
    "\n",
    "## Definition\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying pattern in the data.\n",
    "\n",
    "The model fails to learn the true relationship between input and output.\n",
    "\n",
    "---\n",
    "\n",
    "## Characteristics\n",
    "\n",
    "- High training error\n",
    "- High testing error\n",
    "- Model is too simple\n",
    "- Cannot capture important patterns\n",
    "- High bias\n",
    "- Low variance\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose the true relationship is:\n",
    "\n",
    "y = 0.8x² + 0.9x + 2\n",
    "\n",
    "But we fit a linear model:\n",
    "\n",
    "y = β₀ + β₁x\n",
    "\n",
    "Since a straight line cannot model a quadratic curve,  \n",
    "the model will perform poorly on both training and testing data.\n",
    "\n",
    "Example errors:\n",
    "\n",
    "Training error = 40  \n",
    "Testing error = 45  \n",
    "\n",
    "This is underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Why It Happens\n",
    "\n",
    "- Model is too simple\n",
    "- Not enough features\n",
    "- Degree too low (in polynomial regression)\n",
    "- Excessive regularization\n",
    "- Insufficient training\n",
    "\n",
    "---\n",
    "\n",
    "## How to Fix Underfitting\n",
    "\n",
    "- Increase model complexity\n",
    "- Add more features\n",
    "- Increase polynomial degree\n",
    "- Reduce regularization\n",
    "- Train longer\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Overfitting\n",
    "\n",
    "## Definition\n",
    "\n",
    "Overfitting occurs when a model learns not only the true pattern but also the noise in the training data.\n",
    "\n",
    "The model performs very well on training data but poorly on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Characteristics\n",
    "\n",
    "- Very low training error\n",
    "- High testing error\n",
    "- Model is too complex\n",
    "- Learns noise\n",
    "- Low bias\n",
    "- High variance\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Using a polynomial degree of 30 for quadratic data:\n",
    "\n",
    "Training error = 2  \n",
    "Testing error = 25  \n",
    "\n",
    "The model fits every training point closely,  \n",
    "but fails to generalize.\n",
    "\n",
    "This is overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Why It Happens\n",
    "\n",
    "- Model is too complex\n",
    "- Too many parameters\n",
    "- Small dataset\n",
    "- High polynomial degree\n",
    "- No regularization\n",
    "\n",
    "---\n",
    "\n",
    "## How to Fix Overfitting\n",
    "\n",
    "- Reduce model complexity\n",
    "- Lower polynomial degree\n",
    "- Use regularization (Ridge, Lasso)\n",
    "- Increase training data\n",
    "- Use cross-validation\n",
    "- Apply early stopping\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Comparison Table\n",
    "\n",
    "| Problem       | Training Error | Testing Error | Bias | Variance |\n",
    "|--------------|---------------|--------------|------|----------|\n",
    "| Underfitting | High          | High         | High | Low      |\n",
    "| Good Fit     | Low           | Low          | Balanced | Balanced |\n",
    "| Overfitting  | Very Low      | High         | Low  | High     |\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Visual Intuition\n",
    "\n",
    "- Underfitting → Model too simple (straight line for curved data)\n",
    "- Good Fit → Smooth curve matching pattern\n",
    "- Overfitting → Extremely wiggly curve following noise\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Key Insight\n",
    "\n",
    "The goal in Machine Learning is not to minimize training error,  \n",
    "but to minimize test (generalization) error.\n",
    "\n",
    "That requires balancing bias and variance.\n",
    "\n",
    "This balance is known as the Bias–Variance Tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4df3a-05d0-44d4-a285-4038b598db74",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e95ed6-670e-45c0-91cf-985124d7a7cf",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac364464-74c8-4e40-9367-aca679a18230",
   "metadata": {},
   "source": [
    "##### you have to go for : low -bias and low variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
