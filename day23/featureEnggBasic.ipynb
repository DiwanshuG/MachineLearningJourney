{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49f988b-a2ec-49b9-8db2-2ede2bc3494a",
   "metadata": {},
   "source": [
    "## Feature Engineering – Definition\r\n",
    "\r\n",
    "Feature Engineering is the process of transforming raw data into meaningful and structured features that can be effectively used by machine learning models to learn patterns and make accurate predictions.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d3a45-fb74-4249-ae60-c3a61bf249ee",
   "metadata": {},
   "source": [
    "### Feature Engineering Architecture\n",
    "\n",
    "![Feature Engineering Architecture](architecture.png)\n",
    "\n",
    "This diagram shows the basic feature engineering pipeline from raw data to\n",
    "model-ready features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b48ae-ce2c-43b5-b15a-1460c7564430",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "Feature Transformation refers to modifying existing features to make them more suitable for machine learning models.  \n",
    "It changes the **representation or scale** of data without creating new information.\n",
    "\n",
    "**Examples:**\n",
    "- Scaling (StandardScaler, MinMaxScaler)\n",
    "- Normalization\n",
    "- Log / Power transformations\n",
    "- Encoding categorical variables\n",
    "\n",
    "**Goal:** Improve model performance and convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Construction\n",
    "Feature Construction is the process of **creating new features** from existing ones using domain knowledge or mathematical operations.\n",
    "\n",
    "**Examples:**\n",
    "- Creating `age` from `date_of_birth`\n",
    "- Combining features (`total_amount = price × quantity`)\n",
    "- Polynomial features\n",
    "- Interaction terms\n",
    "\n",
    "**Goal:** Add new information that helps the model learn better patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection\n",
    "Feature Selection is the process of **choosing the most relevant features** and removing redundant or irrelevant ones.\n",
    "\n",
    "**Examples:**\n",
    "- Correlation-based selection\n",
    "- Variance threshold\n",
    "- Recursive Feature Elimination (RFE)\n",
    "- Feature importance from models\n",
    "\n",
    "**Goal:** Reduce overfitting, improve interpretability, and speed up training.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Extraction\n",
    "Feature Extraction transforms raw data into a **new feature space**, often reducing dimensionality.\n",
    "\n",
    "**Examples:**\n",
    "- PCA\n",
    "- LDA\n",
    "- Autoencoders\n",
    "- TF-IDF (for text)\n",
    "\n",
    "**Goal:** Capture important patterns while reducing data complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences (Summary)\n",
    "\n",
    "| Technique | Creates New Features | Removes Features | Changes Representation |\n",
    "|---------|---------------------|------------------|------------------------|\n",
    "| Feature Transformation | No | No | Yes |\n",
    "| Feature Construction | Yes | No | No |\n",
    "| Feature Selection | No | Yes | No |\n",
    "| Feature Extraction | Yes | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f76d7-8889-4608-b77a-6bf3cb3d2e3a",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83e10f-05af-4d34-ac33-3461392c02ea",
   "metadata": {},
   "source": [
    "### 1. missing values treatment\n",
    "1) remove missing values \n",
    "2) filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7db60-e8fc-4872-8e0a-d541b65e5c0e",
   "metadata": {},
   "source": [
    "### Missing Value Filling Techniques\n",
    "\n",
    "Missing values can negatively impact machine learning models. Below are commonly used techniques to fill missing data, chosen based on data type and problem context.\n",
    "\n",
    "---\n",
    "\n",
    "### a. Mean Imputation\n",
    "Replaces missing values with the mean of the feature.\n",
    "\n",
    "**Best suited for:**\n",
    "- Numerical data\n",
    "- Data without extreme outliers\n",
    "\n",
    "**Limitation:**\n",
    "- Sensitive to outliers\n",
    "\n",
    "```python\n",
    "df[\"column\"].fillna(df[\"column\"].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df8872-49bb-4b2f-9377-84adad86bebb",
   "metadata": {},
   "source": [
    "### b. Median Imputation\n",
    "\n",
    "Replaces missing values with the median of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b0ca5-a079-4307-bde6-44ea4a35c427",
   "metadata": {},
   "source": [
    "Best suited for:\n",
    "\n",
    "1) Numerical data with outliers\n",
    "\n",
    "2) Skewed distributions\n",
    "Advantage:\n",
    "Robust to outliers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fff39539-f932-4bc4-a0fe-973f37cf7aea",
   "metadata": {},
   "source": [
    "df[\"column\"].fillna(df[\"column\"].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44963b2-2564-401f-8341-c08034e0c5de",
   "metadata": {},
   "source": [
    "### c. Mode Imputation\n",
    "Replaces missing values with the most frequent value.\n",
    "\n",
    "Best suited for:\n",
    "1) Categorical features\n",
    "\n",
    "Limitation:\n",
    "May introduce bias if one category dominates"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb5f634-d76a-4234-a5c5-49ccb2b1aeae",
   "metadata": {},
   "source": [
    "df[\"column\"].fillna(df[\"column\"].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee25fa-2ee5-41f1-ac77-7201b92d35b7",
   "metadata": {},
   "source": [
    "### d. Forward Fill (ffill)\n",
    "\n",
    "Fills missing values using the previous valid observation.\n",
    "\n",
    "Best suited for:\n",
    "\n",
    "Time-series data\n",
    "\n",
    "Sequential data where order matters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8676755d-3aa3-4880-bcd8-6a519b2e74d8",
   "metadata": {},
   "source": [
    "df[\"column\"].fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299feb50-9227-4fb6-8ec1-bc058c025b64",
   "metadata": {},
   "source": [
    "### e. Backward Fill (bfill)\n",
    "\n",
    "Fills missing values using the next valid observation.\n",
    "\n",
    "Best suited for:\n",
    "1) Time-series data\n",
    "2) Sequential data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47419db3-a25b-48ec-9066-4caa46c0377a",
   "metadata": {},
   "source": [
    "df[\"column\"].fillna(method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662e4d4-4ee5-4709-9ee0-71ec4a097def",
   "metadata": {},
   "source": [
    "## 2. Handling Categorical Values\r\n",
    "\r\n",
    "Categorical variables represent qualitative data and must be converted into numerical form before being used in most machine learning models.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Types of Categorical Variables\r\n",
    "- **Nominal**: Categories with no inherent order  \r\n",
    "  (e.g., color, city, gender)\r\n",
    "- **Ordinal**: Categories with a meaningful order  \r\n",
    "  (e.g., low, medium, high)\r",
    "a\r\n",
    "---\r\n",
    "\r\n",
    "### 1. Label Encoding\r\n",
    "Assigns a unique numerical value to each category.\r\n",
    "\r\n",
    "**Best suited for:**\r\n",
    "- Ordinal categorical variables\r\n",
    "\r\n",
    "**Limitation:**\r\n",
    "- Imposes an artificial order on nominal data\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "le = LabelEncoder()\r\n",
    "df[\"column\"] = le.fit_transform(df[\"column\"])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3a78e-769b-4851-a8bc-1b8473942d41",
   "metadata": {},
   "source": [
    "### b. One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding creates separate binary (0/1) columns for each category in a categorical feature.\n",
    "\n",
    "**Best suited for:**\n",
    "- Nominal categorical variables\n",
    "- Algorithms that assume no order between categories\n",
    "\n",
    "**Limitation:**\n",
    "- Can significantly increase dimensionality when the number of categories is large\n",
    "\n",
    "```python\n",
    "pd.get_dummies(df, columns=[\"column\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76dc47-b4f1-43f7-b59b-327f0c2aedff",
   "metadata": {},
   "source": [
    "### c. Ordinal Encoding\n",
    "\n",
    "Ordinal Encoding converts categorical values into numerical values based on a predefined order.\n",
    "\n",
    "**Best suited for:**\n",
    "- Ordered categorical variables\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[[\"low\", \"medium\", \"high\"]])\n",
    "df[\"column\"] = encoder.fit_transform(df[[\"column\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bcd48-0a64-4408-aba1-faf0a4f69e76",
   "metadata": {},
   "source": [
    "### d. Frequency Encoding\n",
    "Frequency Encoding replaces each category with its frequency count in the dataset.\n",
    "\n",
    "Best suited for:\n",
    "1) High-cardinality categorical features\n",
    "```python\n",
    "freq = df[\"column\"].value_counts()\n",
    "df[\"column\"] = df[\"column\"].map(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88aba2-9580-4f8c-a3c3-0b032b96cc0f",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e698ad8-a8a7-40e8-baf8-1bf232fe6bbf",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection\n",
    "\n",
    "Outliers are data points that significantly deviate from the majority of observations. They can distort statistical analysis and negatively impact the performance of many machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Outlier Detection is Important\n",
    "- Prevents skewed model learning\n",
    "- Improves model stability and accuracy\n",
    "- Helps identify data entry errors or rare but important events\n",
    "\n",
    "---\n",
    "\n",
    "### a. Z-Score Method\n",
    "\n",
    "Measures how many standard deviations a data point is from the mean.\n",
    "\n",
    "**Best suited for:**\n",
    "- Normally distributed data\n",
    "\n",
    "**Limitation:**\n",
    "- Sensitive to outliers themselves\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "z_scores = stats.zscore(df[\"column\"])\n",
    "df_outliers = df[abs(z_scores) > 3]\n",
    "\n",
    "```\n",
    "### b. IQR (Interquartile Range) Method\n",
    "Uses the spread between the 25th and 75th percentiles.\n",
    "\n",
    "### c. Box Plot Method\n",
    "\n",
    "### d.Percentile-Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c42407-4a48-4e2e-a2b4-3a43da296fd0",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature Scaling is the process of standardizing or normalizing numerical features so that they are on a similar scale. This prevents features with larger magnitudes from dominating model learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Feature Scaling is Important\n",
    "- Ensures fair contribution of all features\n",
    "- Improves model convergence\n",
    "- Essential for distance-based and gradient-based algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### Models That REQUIRE Scaling\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks\n",
    "- K-Means Clustering\n",
    "\n",
    "### Models That DO NOT Require Scaling\n",
    "- Decision\n",
    "\n",
    "\n",
    "| Scaling Method | Handles Outliers | Output Range |\r\n",
    "| -------------- | ---------------- | ------------ |\r\n",
    "| StandardScaler | No               | Unbounded    |\r\n",
    "| MinMaxScaler   | No               | 0 to 1       |\r\n",
    "| RobustScaler   | Yes              | Unbounded    |\r\n",
    "| MaxAbsScaler   | Partial          | -1 to 1      |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8315683-ee9c-419c-9ead-3a1c2f968a19",
   "metadata": {},
   "source": [
    "# Feature Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fade5-777a-4e2e-8c9e-863cffc3319c",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "Feature Construction is the process of **creating new features** from existing data using domain knowledge, mathematical operations, or logical rules to improve a model’s ability to learn patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Feature Construction is Important\n",
    "\n",
    "* Adds new information not explicitly present in raw data\n",
    "* Improves model performance without changing algorithms\n",
    "* Helps capture real-world relationships between variables\n",
    "\n",
    "---\n",
    "\n",
    "### Common Feature Construction Techniques\n",
    "\n",
    "#### 1. Mathematical Operations\n",
    "\n",
    "Create new features using arithmetic combinations.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Sum, difference, ratio, product\n",
    "\n",
    "```python\n",
    "df[\"total_price\"] = df[\"price\"] * df[\"quantity\"]\n",
    "df[\"price_per_unit\"] = df[\"total_price\"] / df[\"units\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Polynomial Features\n",
    "\n",
    "Generate interaction and higher-degree terms.\n",
    "\n",
    "**Best suited for:**\n",
    "\n",
    "* Linear models capturing non-linear relationships\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Date and Time Features\n",
    "\n",
    "Extract meaningful components from datetime columns.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Year, month, day, weekday, hour\n",
    "\n",
    "```python\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Aggregation-Based Features\n",
    "\n",
    "Create features using group-level statistics.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Mean, median, count per group\n",
    "\n",
    "```python\n",
    "df[\"avg_salary_by_dept\"] = df.groupby(\"department\")[\"salary\"].transform(\"mean\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Flag / Indicator Features\n",
    "\n",
    "Create binary features to represent conditions.\n",
    "\n",
    "```python\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Text-Derived Features (Basic)\n",
    "\n",
    "Create features from text length or counts.\n",
    "\n",
    "```python\n",
    "df[\"text_length\"] = df[\"review\"].str.len()\n",
    "df[\"word_count\"] = df[\"review\"].str.split().apply(len)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Use domain knowledge whenever possible\n",
    "* Avoid creating too many features blindly\n",
    "* Validate feature usefulness with EDA or model performance\n",
    "* Watch out for data leakage\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Feature Construction creates **new information**\n",
    "* Often more impactful than model tuning\n",
    "* Quality matters more than quantity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd140c-e6f7-4b58-a19a-b477b1c4810e",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddf318d-4ff2-4993-b55d-8c2b7a381561",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f690b14-6900-472a-8bb5-3c817c5a1bd0",
   "metadata": {},
   "source": [
    "Feature Selection is the process of selecting a subset of the most relevant features from the dataset and removing redundant or irrelevant features before training a machine learning model.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Feature Selection is Important\n",
    "\n",
    "* Reduces overfitting by removing noise\n",
    "* Improves model performance and generalization\n",
    "* Decreases training time and computational cost\n",
    "* Improves model interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Feature Selection Methods\n",
    "\n",
    "#### 1. Filter Methods\n",
    "\n",
    "Filter methods select features based on statistical measures, independent of any machine learning model.\n",
    "\n",
    "**Common techniques:**\n",
    "\n",
    "* Correlation analysis\n",
    "* Variance threshold\n",
    "* Chi-square test\n",
    "\n",
    "```python\n",
    "# Correlation-based removal\n",
    "corr_matrix = df.corr()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wrapper Methods\n",
    "\n",
    "Wrapper methods evaluate feature subsets by training a model and measuring performance.\n",
    "\n",
    "**Common techniques:**\n",
    "\n",
    "* Forward Selection\n",
    "* Backward Elimination\n",
    "* Recursive Feature Elimination (RFE)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "X_selected = rfe.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Embedded Methods\n",
    "\n",
    "Embedded methods perform feature selection as part of the model training process.\n",
    "\n",
    "**Common techniques:**\n",
    "\n",
    "* Lasso (L1 regularization)\n",
    "* Tree-based feature importance\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "importances = model.feature_importances_\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Feature Selection\n",
    "\n",
    "* High-dimensional datasets\n",
    "* Presence of multicollinearity\n",
    "* Limited computational resources\n",
    "* Need for model interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "* Perform feature selection after train-test split\n",
    "* Combine domain knowledge with statistical methods\n",
    "* Avoid removing features blindly\n",
    "* Validate selected features using cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* Feature Selection removes irrelevant features\n",
    "* Helps simplify models and improve generalization\n",
    "* Different methods suit different problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84933135-cb80-4adf-a012-7164a6c66648",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede505b-0b92-4760-a6ea-ac38f4a022bd",
   "metadata": {},
   "source": [
    "Feature Extraction is the process of transforming raw or high-dimensional data into a new set of features that captures the most important information while reducing complexity. It creates a new feature space that is more suitable for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Feature Extraction is Important\n",
    "- Reduces dimensionality of data\n",
    "- Removes redundant and noisy information\n",
    "- Improves model performance and training speed\n",
    "- Helps handle high-dimensional datasets\n",
    "\n",
    "---\n",
    "\n",
    "### Common Feature Extraction Techniques\n",
    "\n",
    "#### a. Principal Component Analysis (PCA)\n",
    "PCA converts correlated numerical features into a smaller set of uncorrelated components while preserving maximum variance.\n",
    "\n",
    "**Best suited for:**\n",
    "- Numerical data\n",
    "- Dimensionality reduction\n",
    "- Multicollinearity issues\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 2. Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA projects data into a lower-dimensional space while maximizing class separability.\n",
    "\n",
    "Best suited for:\n",
    "**Supervised classification problems**\n",
    "\n",
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "```\n",
    "\n",
    "### 3. Text Feature Extraction\n",
    "\n",
    "Transforms unstructured text into numerical features.\n",
    "Common techniques:\n",
    "1) Bag of Words\n",
    "2) TF-IDF\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_text = vectorizer.fit_transform(text_data)\n",
    "```\n",
    "### d. Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that learn compressed representations of data.\n",
    "\n",
    "Best suited for:\n",
    "\n",
    "1) Large-scale datasets\n",
    "2) Deep learning applications\n",
    "\n",
    "\n",
    "| Aspect                   | Feature Selection | Feature Extraction |\r\n",
    "| ------------------------ | ----------------- | ------------------ |\r\n",
    "| Creates new features     | No                | Yes                |\r\n",
    "| Removes features         | Yes               | Yes                |\r\n",
    "| Dimensionality reduction | Partial           | Strong             |\r\n",
    "| Interpretability         | High              | Lower           \n",
    "\n",
    "**Best Practices**\n",
    "\n",
    "1) Apply feature extraction after scaling (for PCA and LDA)\n",
    "2) Choose the number of components carefully\n",
    "3) Perform extraction after train-test split\n",
    "4) Validate impact on model performance\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1) Feature Extraction creates a new feature space\n",
    "2) It is useful for high-dimensional and complex data\n",
    "3) Improves efficiency but reduces interpretability   |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7b590-4706-4936-9741-abe31585e202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
