{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6d580ab-b43c-4fbc-943e-624e65c883e1",
   "metadata": {},
   "source": [
    "feature extraction -> PCA -> f [decreases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a25b92-30dd-4f9e-80ec-7b909a7636bc",
   "metadata": {},
   "source": [
    "The problem PCA is trying to solve : \n",
    "In real datasets:\n",
    "1) We have many features\n",
    "2) Many of them are correlated\n",
    "3) High dimensions : \n",
    "    > slow models\n",
    "    \n",
    "    > overfitting\n",
    "    \n",
    "    > hard visualization\n",
    "    \n",
    "    > curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8357ef-d4c2-403e-b95d-c10b677f3ed7",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that: \n",
    ">Finds new axes (directions) in data\n",
    "\n",
    ">These axes capture maximum variance\n",
    "\n",
    ">And are uncorrelated with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818aa3a-f021-4b77-ba79-02647ac212e3",
   "metadata": {},
   "source": [
    "### Geometric Intuition : "
   ]
  },
  {
   "cell_type": "raw",
   "id": "379c77e6-3627-4b4d-9739-dce015d31db9",
   "metadata": {},
   "source": [
    "feature selection : we choose the inportant feature for output prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0e2d6-4ec8-47f2-aaa4-bc527301c810",
   "metadata": {},
   "source": [
    ">when you have very big dataset , its kind of hard to predict which feature should we select "
   ]
  },
  {
   "cell_type": "raw",
   "id": "36341b4c-4195-4c32-b2d9-a6216d34219d",
   "metadata": {},
   "source": [
    "for example we have two feature and we have to select one : we will check the data spread of each relative axis and choose the datset which has the big data spread on its axis [axis with high variance]\n",
    "\n",
    "\n",
    "when two data spread [variance] is equal : then select which one ? : here we use feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3516c0-34c3-476b-a9c2-2a5639b3d210",
   "metadata": {},
   "source": [
    "### PCA Steps\n",
    "\n",
    "1. Collect the dataset with numerical features.\n",
    "2. Clean the data (handle missing values and duplicates).\n",
    "3. Standardize all features so they are on the same scale.\n",
    "4. Compute the covariance matrix to understand relationships between features.\n",
    "5. Calculate eigenvalues and eigenvectors of the covariance matrix.\n",
    "6. Sort eigenvalues in descending order based on explained variance.\n",
    "7. Select the top k principal components.\n",
    "8. Project the original data onto the new component axes.\n",
    "9. Analyze explained variance to check information retention.\n",
    "10. Use the PCA-transformed data for visualization or modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd93d7d-8099-4c62-af48-ed5855445c4e",
   "metadata": {},
   "source": [
    "##### we make a new column from combination of other columns -> feature constuction [in PCA]\n",
    "##### PCA never chooses between features — it creates better ones."
   ]
  },
  {
   "cell_type": "raw",
   "id": "041f2ded-9881-40b7-b7c5-a276025349ce",
   "metadata": {},
   "source": [
    "pca finds a new coordinate axis so that the variance increases : new coordinate system \n",
    "\n",
    "'these new axes are known as Principal components [pca1 , pca2 ,.....]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87103b01-d978-483c-8bfb-df5cf60ce484",
   "metadata": {},
   "source": [
    "> If two features have different variance, we keep the one with higher variance.\n",
    "If they have equal variance, PCA does not select one feature; instead, it creates a new feature by rotating the axes to capture maximum variance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ade1f71-ec60-4c59-997a-709ec9ae737a",
   "metadata": {},
   "source": [
    "No. of principal component <= n(original features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0fc1e-4272-4871-a9af-9c87eb2795d4",
   "metadata": {},
   "source": [
    "### Variance — Why is it Important?\n",
    "\n",
    "Variance measures how much the data spreads out from its mean.\n",
    "\n",
    "In machine learning, variance is important because:\n",
    "- High variance indicates more information and diversity in the data.\n",
    "- Low variance often means the feature is almost constant and carries little useful information.\n",
    "- Features with zero or near-zero variance do not help models learn patterns.\n",
    "\n",
    "In PCA:\n",
    "- Variance is treated as a proxy for information.\n",
    "- Directions with high variance capture the main structure of the data.\n",
    "- Directions with low variance mostly represent noise.\n",
    "\n",
    "Geometric intuition:\n",
    "- A wider data spread means better separation between points.\n",
    "- PCA keeps directions where points are far apart and discards directions where points collapse.\n",
    "\n",
    "In simple terms:\n",
    "- High variance = strong signal\n",
    "- Low variance = weak signal or noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33970b7c-581a-4b74-8936-681381f6dc3d",
   "metadata": {},
   "source": [
    "### Variance (Mathematical Definition)\n",
    "\n",
    "Variance measures the average squared deviation of data points from their mean.\n",
    "\n",
    "For a dataset:\n",
    "x₁, x₂, x₃, ..., xₙ\n",
    "\n",
    "1. Compute the mean (average):\n",
    "μ = (1/n) Σ xᵢ\n",
    "\n",
    "2. Compute deviation from the mean:\n",
    "(xᵢ − μ)\n",
    "\n",
    "3. Square the deviations:\n",
    "(xᵢ − μ)²\n",
    "\n",
    "4. Take the average of squared deviations:\n",
    "\n",
    "Population Variance:\n",
    "σ² = (1/n) Σ (xᵢ − μ)²\n",
    "\n",
    "Sample Variance:\n",
    "s² = (1/(n − 1)) Σ (xᵢ − μ)²\n",
    "\n",
    "Why square the deviations?\n",
    "- Prevents positive and negative values from canceling out\n",
    "- Penalizes larger deviations more\n",
    "- Makes variance mathematically convenient for optimization\n",
    "\n",
    "In PCA:\n",
    "- Variance along a direction tells how much information is captured\n",
    "- PCA finds directions that maximize σ²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027307a-2aad-4510-be39-1ca4c1de8fec",
   "metadata": {},
   "source": [
    "### Spread vs Variance (Important Distinction)\n",
    "\n",
    "Spread is a qualitative, geometric idea.\n",
    "Variance is a quantitative, mathematical measure.\n",
    "\n",
    "Variance does NOT equal spread.\n",
    "Variance is proportional to the square of the spread.\n",
    "\n",
    "Mathematically:\n",
    "Variance = average of (distance from mean)²\n",
    "\n",
    "If data points are twice as far from the mean,\n",
    "the variance becomes four times larger.\n",
    "\n",
    "So:\n",
    "- Larger spread → larger variance\n",
    "- Smaller spread → smaller variance\n",
    "- But they are not the same thing\n",
    "\n",
    "Why PCA uses variance:\n",
    "- Variance provides a precise, computable measure of spread\n",
    "- Squaring emphasizes larger deviations\n",
    "- Makes optimization and linear algebra tractable\n",
    "\n",
    "Correct way to say it:\n",
    "\"Variance is a mathematical quantity that is proportional to the square of the data spread.\"\n",
    "\n",
    "PCA intuition:\n",
    "- PCA finds directions with maximum variance\n",
    "- Which corresponds to directions with maximum data spread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1d22e-ec95-46d1-8f9f-24905af051d1",
   "metadata": {},
   "source": [
    "##### Is the modulus (absolute value) function differentiable at zero?\n",
    "\n",
    "The modulus function is:\n",
    "f(x) = |x|\n",
    "\n",
    "For x > 0:\n",
    "f′(x) = 1\n",
    "\n",
    "For x < 0:\n",
    "f′(x) = −1\n",
    "\n",
    "At x = 0:\n",
    "- Left-hand derivative = −1\n",
    "- Right-hand derivative = +1\n",
    "\n",
    "Since the left-hand and right-hand derivatives are not equal,\n",
    "the derivative at x = 0 does NOT exist.\n",
    "\n",
    "Therefore:\n",
    "The modulus (absolute value) function is NOT differentiable at zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02964b5c-220d-4cea-b44b-3991874d5047",
   "metadata": {},
   "source": [
    ">ML prefers variance over absolute spread because squared loss is smooth and differentiable, enabling efficient optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8dbbde-3a5b-4a09-a1ef-de8504365323",
   "metadata": {},
   "source": [
    ">A good analogy is viewing the world from the right camera angle:\n",
    "Just like a photographer chooses the best angle to capture the most meaningful view of a scene, PCA chooses the direction of highest variance so that when we reduce dimensions we retain as much of the important structure in the data as possible. This is why we say PCA tries to retain most of the variance while reducing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a503bb-f597-413d-8d61-b5baabb61c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
