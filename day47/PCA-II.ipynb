{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc4ab80-a812-44af-86d0-de59494d39c4",
   "metadata": {},
   "source": [
    "#### Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cada85-cb28-465a-82ca-9626e4c4c6ec",
   "metadata": {},
   "source": [
    "Given\n",
    "\n",
    "1) Each data point is a vector\n",
    "2) Example:\n",
    "3) A point x = (x1,x2) can be seen as a vector in 2D space\n",
    "\n",
    "4) We have many such vectors (the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5673b-9810-406f-b817-39a227b1583c",
   "metadata": {},
   "source": [
    "### Projection of one vector onto another (origin-based)\r\n",
    "\r\n",
    "Given two points from the origin:\r\n",
    "- Vector **v** = (x₁, y₁)\r\n",
    "- Vector **u** = (x₂, y₂)\r\n",
    "\r\n",
    "We want the projection of **v** onto **u**.\r\n",
    "\r\n",
    "#### Step 1: Dot products\r\n",
    "v · u = x₁x₂ + y₁y₂  \r\n",
    "u · u = x₂² + y₂²\r\n",
    "\r\n",
    "#### Step 2: Projection formula\r\n",
    "The projection of **v** onto **u** is:\r\n",
    "\r\n",
    "projᵤ(v) = ( (v · u) / (u · u) ) · u\r\n",
    "\r\n",
    "#### Step 3: Write it explicitly\r\n",
    "projᵤ(v) = ( (x₁x₂ + y₁y₂) / (x₂² + y₂²) ) · (x₂, y₂)\r\n",
    "\r\n",
    "= ( ((x₁x₂ + y₁y₂)x₂) / (x₂² + y₂²),\r\n",
    "    ((x₁x₂ + y₁y₂)y₂) / (x₂² + y₂²) )\r\n",
    "\r\n",
    "#### Interpretation\r\n",
    "- This gives the **shadow of (x₁, y₁)** along the direction **(x₂, y₂)**.\r\n",
    "- If v is perpendicular to u → projection is (0, 0).\r\n",
    "- If v is parallel to u → projection equals v (scaled).\r\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec1839cc-0477-49c9-9587-656e0c73207b",
   "metadata": {},
   "source": [
    "After projecting each data point onto a chosen direction vector,\n",
    "each point is represented by a single scalar value (its coordinate along that direction).\n",
    "\n",
    "Different projection directions produce different sets of scalar values.\n",
    "\n",
    "Our objective is to choose the projection direction such that\n",
    "the variance of these projected scalar values is maximized.\n",
    "\n",
    "This direction captures the maximum spread of the data\n",
    "and is selected as the first principal component.\n",
    "\n",
    "mponent.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1cc68-6cc6-464d-9967-63f71b7d1fba",
   "metadata": {},
   "source": [
    "### Variance of Projected Data (Same Example)\n",
    "\n",
    "Assume we have n data points:\n",
    "x₁, x₂, ..., xₙ  \n",
    "Each point xᵢ is a vector in ℝ² (or ℝᵈ).\n",
    "\n",
    "Let u be a unit vector (projection direction).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Projection gives scalar values\n",
    "\n",
    "Project each data point xᵢ onto u:\n",
    "\n",
    "zᵢ = xᵢ · u\n",
    "\n",
    "So after projection:\n",
    "- Each vector → one scalar value\n",
    "- Dataset becomes: z₁, z₂, ..., zₙ\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Mean of projected values\n",
    "\n",
    "μ_z = (1/n) Σ (xᵢ · u)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Variance of projected data\n",
    "\n",
    "Variance is defined as:\n",
    "\n",
    "Var(z) = (1/n) Σ (zᵢ − μ_z)²\n",
    "\n",
    "Substitute zᵢ = xᵢ · u:\n",
    "\n",
    "Var(z) = (1/n) Σ [ (xᵢ · u − μ_z)² ]\n",
    "\n",
    "This is the variance **along direction u**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: PCA objective\n",
    "\n",
    "We choose the direction u such that:\n",
    "\n",
    "Var(z) is maximized\n",
    "\n",
    "i.e.,\n",
    "\n",
    "maximize  (1/n) Σ (xᵢ · u − μ_z)²  \n",
    "subject to  ||u|| = 1\n",
    "\n",
    "This optimal direction u becomes\n",
    "the **first principal component (PC1)**.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf20a506-26d8-4c12-9187-755913940867",
   "metadata": {},
   "source": [
    "Variance measures how much a single variable spreads around its mean,\n",
    "but it does not tell us anything about the relationship between two variables.\n",
    "\n",
    "To understand how two variables vary together,\n",
    "we use covariance.\n",
    "\n",
    "Covariance captures:\n",
    "- whether X and Y increase or decrease together\n",
    "- whether they move in opposite directions\n",
    "- or whether there is no linear relationship at all\n",
    "\n",
    "In PCA:\n",
    "- Variance alone is not sufficient because data structure depends on\n",
    "  relationships between features.\n",
    "- Covariance allows us to capture both individual spreads (variances)\n",
    "  and inter-feature relationships.\n",
    "- PCA uses the covariance matrix to find directions where the combined\n",
    "  variance of all features is maximized.\n",
    "\n",
    "In short:\n",
    "Variance → spread of a single feature  \n",
    "Covariance → relationship between two features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de281130-63e1-4104-b4d3-de65f117c032",
   "metadata": {},
   "source": [
    "### Covariance (What & Why)\n",
    "\n",
    "Covariance measures how two variables change together.\n",
    "\n",
    "Given two features X and Y with n observations:\n",
    "\n",
    "Cov(X, Y) = (1/n) Σ (xᵢ − μₓ)(yᵢ − μᵧ)\n",
    "\n",
    "where:\n",
    "- μₓ = mean of X\n",
    "- μᵧ = mean of Y\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Cov(X, Y) > 0  \n",
    "→ X and Y increase together (positive relationship)\n",
    "\n",
    "Cov(X, Y) < 0  \n",
    "→ One increases, the other decreases (negative relationship)\n",
    "\n",
    "Cov(X, Y) = 0  \n",
    "→ No linear relationship\n",
    "\n",
    "---\n",
    "\n",
    "### Relation to Variance\n",
    "\n",
    "Variance is a special case of covariance:\n",
    "\n",
    "Var(X) = Cov(X, X)\n",
    "\n",
    "So:\n",
    "- Variance → spread of one variable\n",
    "- Covariance → joint spread of two variables\n",
    "\n",
    "---\n",
    "\n",
    "### Covariance Matrix\n",
    "\n",
    "For a dataset with d features:\n",
    "\n",
    "Σ =\n",
    "[ Cov(X₁, X₁)  Cov(X₁, X₂)  ... ]\n",
    "[ Cov(X₂, X₁)  Cov(X₂, X₂)  ... ]\n",
    "[      .            .      .  ]\n",
    "\n",
    "- Diagonal elements → variances\n",
    "- Off-diagonal elements → covariances\n",
    "\n",
    "---\n",
    "\n",
    "### Why Covariance is Crucial for PCA\n",
    "\n",
    "- PCA looks for directions where variance is maximized\n",
    "- Variance along a direction depends on how features vary together\n",
    "- Covariance matrix captures all variance + relationships at once\n",
    "\n",
    "Key result:\n",
    "Variance after projection onto direction u is:\n",
    "\n",
    "Var(z) = uᵀ Σ u\n",
    "\n",
    "PCA chooses u that maximizes this quantity.\n",
    "\n",
    "---\n",
    "\n",
    "###  \n",
    "> Covariance tells us how features move together, and PCA uses this information to find directions of maximum variance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd86fb5c-1927-4efb-bc5d-af00777949e9",
   "metadata": {},
   "source": [
    "- Covariance Matrix\n",
    "\n",
    "For a dataset with n samples and d features:\n",
    "\n",
    "X = [\n",
    "  [x₁₁, x₁₂, ..., x₁d],\n",
    "  [x₂₁, x₂₂, ..., x₂d],\n",
    "  ...\n",
    "  [xₙ₁, xₙ₂, ..., xₙd]\n",
    "]\n",
    "\n",
    "The covariance matrix Σ is a d × d matrix defined as:\n",
    "\n",
    "Σᵢⱼ = Cov(Xᵢ, Xⱼ)\n",
    "\n",
    "where:\n",
    "- Σᵢᵢ → variance of feature i\n",
    "- Σᵢⱼ → covariance between feature i and j\n",
    "\n",
    "\n",
    "Σ = [\n",
    "  [ Var(X₁)   Cov(X₁,X₂)  Cov(X₁,X₃) ],\n",
    "  [ Cov(X₂,X₁) Var(X₂)   Cov(X₂,X₃) ],\n",
    "  [ Cov(X₃,X₁) Cov(X₃,X₂) Var(X₃)   ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a64834-48e2-48bf-b5cb-81123885dd35",
   "metadata": {},
   "source": [
    "> The covariance matrix summarizes how all features vary individually and together, and PCA uses its eigenvectors to find directions of maximum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a7d66-f1de-4941-b2ac-1796ab4cdb20",
   "metadata": {},
   "source": [
    "## Linear Transformation ,  Eigen Vector & Eigen Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e3471-1d91-485c-b982-cf883f50fd0b",
   "metadata": {},
   "source": [
    "### Linear Transformation\n",
    "\n",
    "A linear transformation is a function that maps vectors to vectors\n",
    "while preserving linear structure.\n",
    "\n",
    "Mathematically:\n",
    "y = A x\n",
    "\n",
    "where:\n",
    "- x is the input vector\n",
    "- A is a matrix\n",
    "- y is the transformed vector\n",
    "\n",
    "In PCA:\n",
    "- The covariance matrix Σ acts as a linear transformation\n",
    "- It stretches, compresses, and rotates the data space\n",
    "\n",
    "---\n",
    "\n",
    "### Eigenvectors and Eigenvalues\n",
    "\n",
    "An eigenvector is a special vector that does NOT change its direction\n",
    "after a linear transformation.\n",
    "\n",
    "Only its magnitude changes.\n",
    "\n",
    "Mathematically:\n",
    "A v = λ v\n",
    "\n",
    "where:\n",
    "- v is an eigenvector\n",
    "- λ (lambda) is the eigenvalue\n",
    "- A is the transformation matrix\n",
    "\n",
    "Meaning:\n",
    "- The transformation scales v by λ\n",
    "- Direction of v remains the same\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric Interpretation\n",
    "\n",
    "- Eigenvectors → directions that remain fixed under transformation\n",
    "- Eigenvalues → amount of stretching or compression along that direction\n",
    "\n",
    "If:\n",
    "λ > 1 → stretching  \n",
    "0 < λ < 1 → compression  \n",
    "λ = 0 → collapse to zero  \n",
    "λ < 0 → flip + scale  \n",
    "\n",
    "---\n",
    "\n",
    "### Why Eigenvectors Matter in PCA\n",
    "\n",
    "In PCA:\n",
    "- A = covariance matrix Σ\n",
    "- Eigenvectors of Σ → principal component directions\n",
    "- Eigenvalues → variance captured along each direction\n",
    "\n",
    "Key result:\n",
    "Variance along eigenvector v = corresponding eigenvalue λ\n",
    "\n",
    "So:\n",
    "- Largest eigenvalue → maximum variance\n",
    "- Corresponding eigenvector → PC1\n",
    "\n",
    "---\n",
    "\n",
    "### \n",
    "Eigenvectors give the directions of maximum variance,\n",
    "and eigenvalues tell how much variance lies along those directions.\n",
    "\n",
    "---\n",
    "\n",
    "### PCA Pipeline Connection\n",
    "\n",
    "Data → Covariance Matrix → Eigenvectors → Projection → Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31787e3f-a591-477f-a646-c4acbc279d52",
   "metadata": {},
   "source": [
    ">Eigenvectors are directions that remain unchanged (up to scaling) under a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32849c3e-1db2-4f02-86e6-4c0a02ef13a6",
   "metadata": {},
   "source": [
    ">  eigenvector corresponding to the largest eigenvalue of the covariance matrix\n",
    "gives the direction along which the data has the maximum variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4469f75-fe6e-4b1e-ad2f-b60fa93e1157",
   "metadata": {},
   "source": [
    "> so we have to do Eigendecomposition of a Covariance : \n",
    "\n",
    "[ Read blog ](https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#:~:text=covariance%20matrix%20captures%20the%20spread%20of%20N-dimensional%20data.&text=Figure%203.,is%20captured%20by%20the%20variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869847f6-8d56-40b9-aea3-ab8ea1d9e46a",
   "metadata": {},
   "source": [
    "[Visualizing Linear Transformations](https://www.geogebra.org/m/YCZa8TAH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc58594-f73d-4bfb-8ebb-9d89c39d049a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
