{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241c8ca6-8c14-4ee4-952c-72f23075e472",
   "metadata": {},
   "source": [
    "[kaggle notebook]( https://www.kaggle.com/code/mrtaiech/pca-1-demo)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afead001-200e-4132-b6e4-11deff9bb75f",
   "metadata": {},
   "source": [
    "Best Accuracy: 0.9557142857142857\n",
    "Best PCA Components: 85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d233b71-94ba-4b10-bf06-0c11223a6429",
   "metadata": {},
   "source": [
    "### Finding the optimum number of Principle components"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0de06838-de75-4131-a906-e855b667589c",
   "metadata": {},
   "source": [
    "# MNIST data \n",
    "784 dimensions : eigen value1 , eg2 , eg3 ,......, eg784\n",
    "\n",
    "\n",
    "# Eigenvalues tell us how much variance each principal component explains from the original data; that’s why they are also referred to as explained_variance_ in PCA.\n",
    "\n",
    "\n",
    "[eg1/ (eg1 + eg2 + ...... + eg784) ] * 100 -> percentage contribution of eg1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e677c6-8d1e-4189-915d-313bebf2df49",
   "metadata": {},
   "source": [
    "explained_variance_ → absolute variance (eigenvalues)\n",
    "\n",
    "explained_variance_ratio_ → percentage contribution"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55d7731d-fa01-4bfb-bc66-fb9e30dc3e44",
   "metadata": {},
   "source": [
    "λ1 , λ2 , .......... λ784 \n",
    "\n",
    "\n",
    "We retain only those principal components whose cumulative explained variance is approximately 90% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98841219-65b5-4dc7-a22f-b2b8f489f61c",
   "metadata": {},
   "source": [
    "## Steps \n",
    "\n",
    "1) Fit PCA without fixing components\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\r\n",
    "import numpy as np\n",
    "\r\n",
    "pca = PCA()\r\n",
    "pca.fit(X_tr\n",
    "```\n",
    "\n",
    "2) Get explained variance ratio\n",
    "\n",
    "```python\n",
    "explained_var = pca.explained_variance_ratio_\r",
    "```\n",
    "3) Compute cumulative sum\n",
    "\n",
    "```python\n",
    "cumulative_var = np.cumsum(explained_var)\r",
    "```\n",
    "\n",
    "4) Find number of components for 90%\n",
    "```python\n",
    "n_components_90 = np.argmax(cumulative_var >= 0.90) + 1\r\n",
    "print(\"Components needed for ~90% variance:\", n_components_90)```\r\n",
    "\n",
    "in)\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3386233-419c-4d4b-92f5-3d5708dc8fbd",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9d3b339-4e6d-4cae-8b79-465fe5fce701",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_train_trf = pca.fit_transform(X_train)\n",
    "X_test_trf = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cec49c5-2b7f-4558-b9b7-de726d4a890f",
   "metadata": {},
   "source": [
    "pca.explained_variance_.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65eb4f30-ddba-4079-9f5c-0467139185fc",
   "metadata": {},
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50b495aa-2944-4e75-afc3-09e4f2d602db",
   "metadata": {},
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fc685-730c-4f01-916a-8d6236f367f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51a5f52-25a1-4dac-af58-4fdceb34e140",
   "metadata": {},
   "source": [
    "## when PCA does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c64c5b-6e0a-4037-be0a-956920dddc1e",
   "metadata": {},
   "source": [
    "\r\n",
    "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique, but it has several limitations. PCA may not be suitable in the following scenarios:\r\n",
    "\r\n",
    "### 1. Non-linear relationships in data\r\n",
    "PCA is a linear transformation method. If the underlying structure of the data is non-linear (e.g., circular or spiral patterns), PCA fails to capture meaningful structure.\r\n",
    "\r\n",
    "**Alternative:** Kernel PCA, t-SNE, UMAP\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. When class separation is important\r\n",
    "PCA is an unsupervised technique and does not consider class labels. High-variance directions may not correspond to directions that best separate classes.\r\n",
    "\r\n",
    "**Alternative:** Linear Discriminant Analysis (LDA)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. When features are not scaled\r\n",
    "PCA is sensitive to the scale of features. If features are not standardized, variables with larger scales dominate the principal components.\r\n",
    "\r\n",
    "**Solution:** Apply standardization before PCA.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. When interpretability is required\r\n",
    "Principal components are linear combinations of original features, making them difficult to interpret and explain to stakeholders.\r\n",
    "\r\n",
    "**Alternative:** Feature selection methods\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Small dataset with high dimensionality\r\n",
    "When the number of samples is small compared to the number of features, PCA can produce unstable and noisy components due to poor covariance estimation.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. When variance does not represent useful information\r\n",
    "PCA assumes that higher variance corresponds to more information, which may not always be true. Noise can have high variance, while important signals may have low variance.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 7. Sparse or categorical data\r\n",
    "PCA performs poorly on sparse matrices and categorical features (e.g., one-hot encoded data).\r\n",
    "\r\n",
    "**Alternative:** Truncated SVD for sparse data\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Summary\r\n",
    "PCA is most effective for dense, numerical, and linearly correlated data. It should be avoided when data is non-linear, labels are crucial, interpretability matters, or variance does not reflect true information.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855ca71-7187-48ce-8b14-c59f65c4c3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
