{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc70c30-84a9-4b92-91d1-daa7860001de",
   "metadata": {},
   "source": [
    "types of Gradient descent \n",
    "1) Batch Gradient Descent (Full Batch GD)\n",
    "2) Stochastic Gradient Descent (SGD)\n",
    "3) Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31eb801a-f3f4-4d4f-ab4e-60d0a65f7ca1",
   "metadata": {},
   "source": [
    "basis of these three types ??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8237e944-563d-4b63-a7a5-8aa302bfbb6e",
   "metadata": {},
   "source": [
    "in GD you start from random value of m & b , and you interate them by updating value of m & b in each step :\n",
    "using updation rule \n",
    "θ=θ−η∇J(θ)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2656414-46d1-434d-b168-4e54a6d5eb5f",
   "metadata": {},
   "source": [
    "| Method        | Data Used Per Update       | Number of Updates per Epoch |\n",
    "| ------------- | -------------------------- | --------------------------- |\n",
    "| Batch GD      | Entire dataset (n samples) | 1                           |\n",
    "| SGD           | 1 sample                   | n                           |\n",
    "| Mini-Batch GD | b samples                  | n / b                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db47cb2-45dd-445e-882d-35ce61e5f086",
   "metadata": {},
   "source": [
    "##  Mathematical View\n",
    "\n",
    "$$\n",
    "\\textbf{Full Cost Function:}\n",
    "\\quad\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L_i(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{Batch Gradient Descent:}\n",
    "\\quad\n",
    "\\nabla J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla L_i(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{Stochastic Gradient Descent (SGD):}\n",
    "\\quad\n",
    "\\nabla J(\\theta) \\approx \\nabla L_i(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{Mini-Batch Gradient Descent:}\n",
    "\\quad\n",
    "\\nabla J(\\theta) \\approx \\frac{1}{b} \\sum_{i=1}^{b} \\nabla L_i(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3faf6a79-2a5a-4af7-9661-b6dc7a722391",
   "metadata": {},
   "source": [
    "SGD and Mini-Batch Gradient Descent are used most often in modern machine learning, especially in deep learning, because they scale well to very large datasets and high-dimensional models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245c11e-f5a7-4410-99b3-b77644e354c1",
   "metadata": {},
   "source": [
    "Batch Gradient Descent is typically used when:\n",
    "\n",
    "- The dataset is small\n",
    "- The loss function is convex\n",
    "\n",
    "- Stable and deterministic convergence is preferred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c23bf0c-8cdf-49a4-8d11-50e291402b25",
   "metadata": {},
   "source": [
    "for this example : cgpa | iq | gender | lpa"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1553661-67a8-4435-991e-c4369aa9c506",
   "metadata": {},
   "source": [
    "if we have n-Dimensions data then we have to calculate n+1 coefficeint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108d2e4-fdfd-432a-90a7-b01ba6331fc3",
   "metadata": {},
   "source": [
    "### Mathematical formulation of BatchGD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81c497ad-acd8-44cc-8e03-0d559163e96d",
   "metadata": {},
   "source": [
    "let's understand using 3 cols data : cgpa | iq | lpa \n",
    "data.shape : (2,3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a8a700b-7a67-4f11-9738-2b212a41da8e",
   "metadata": {},
   "source": [
    "y(i)=β0​+β1​x1(i)​+β2​x2(i)​"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4adda016-de9a-48bf-831f-5f883172ee28",
   "metadata": {},
   "source": [
    "1) random values : \n",
    "β0​ = 0 , β1 = 1 , β2 = 1\n",
    "2) epoch = 100 , lr =0.1\n",
    "       β0​ = β0​ - lr*slope\n",
    "       β1 = β1 - lr*slope\n",
    "       β2 = β2 - lr*slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17df210-bc06-4aae-8d30-07d3bd744ae3",
   "metadata": {},
   "source": [
    "##### Batch GD Update\n",
    "\n",
    "For each epoch:\n",
    "\n",
    "$$\n",
    "\\beta_0 := \\beta_0 - \\eta \\frac{\\partial J}{\\partial \\beta_0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_1 := \\beta_1 - \\eta \\frac{\\partial J}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_2 := \\beta_2 - \\eta \\frac{\\partial J}{\\partial \\beta_2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c7e20-b405-4c00-9f33-15e54defd226",
   "metadata": {},
   "source": [
    "## Derivation of  ∂J / ∂β₀  (Batch Gradient Descent)\n",
    "\n",
    "### Step 1: Model\n",
    "\n",
    "For multiple linear regression:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Cost Function (MSE)\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta_1, \\beta_2)\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "Substituting the model:\n",
    "\n",
    "$$\n",
    "J =\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i -\n",
    "(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i})\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Differentiate with respect to β₀\n",
    "\n",
    "Let the error be:\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\n",
    "$$\n",
    "\n",
    "Now differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_0}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "2 e_i\n",
    "\\frac{\\partial e_i}{\\partial \\beta_0}\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "e_i = y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i})\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e_i}{\\partial \\beta_0} = -1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Final Result\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_0}\n",
    "=\n",
    "-\\frac{2}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### note\n",
    "\n",
    "- It is proportional to the **average error**\n",
    "- If predictions are too small → gradient becomes negative → β₀ increases\n",
    "- If predictions are too large → gradient becomes positive → β₀ decreases\n",
    "\n",
    "This is how the bias term corrects itself during Batch Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297c40c-ccf9-4c31-8eeb-4d490f4a68c9",
   "metadata": {},
   "source": [
    "##  ∂J / ∂β₁ \n",
    "\n",
    "### Step 1: Model\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Cost Function (MSE)\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta_1, \\beta_2)\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Define Error\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Differentiate w.r.t β₁\n",
    "\n",
    "Using chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_1}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "2 e_i\n",
    "\\frac{\\partial e_i}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "Now compute:\n",
    "\n",
    "$$\n",
    "e_i = y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i})\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e_i}{\\partial \\beta_1} = -x_{1i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Substitute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_1}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "2 e_i (-x_{1i})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Result\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_1}\n",
    "=\n",
    "-\\frac{2}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "x_{1i}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### notes\n",
    "\n",
    "- Gradient is weighted by feature value \\( x_{1i} \\)\n",
    "- If CGPA strongly contributes to error → β₁ adjusts more\n",
    "- Larger feature values produce larger updates\n",
    "\n",
    "This is why **feature scaling is important** in Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034993f-de94-4d8f-9c39-48a091e5ce32",
   "metadata": {},
   "source": [
    "##  ∂J / ∂β₂\n",
    "\n",
    "### Step 1: Model\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Cost Function\n",
    "\n",
    "$$\n",
    "J =\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Define Error\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Differentiate w.r.t β₂\n",
    "\n",
    "Using chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "2 e_i\n",
    "\\frac{\\partial e_i}{\\partial \\beta_2}\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$$\n",
    "e_i = y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i})\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e_i}{\\partial \\beta_2} = -x_{2i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Result\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\n",
    "=\n",
    "-\\frac{2}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "x_{2i}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c289d0-ffbe-4b87-b653-ae82fd105ba5",
   "metadata": {},
   "source": [
    "## Final Pattern\n",
    "\n",
    "for any feature $x_j$\n",
    ":\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_j}\n",
    "=\n",
    "-\\frac{2}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "x_{ji}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "And for bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\beta_0}\n",
    "=\n",
    "-\\frac{2}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\left(\n",
    "y_i - \\hat{y}_i\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc8b53-b527-47e6-be0a-9b488450d864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
