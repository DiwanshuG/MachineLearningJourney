{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b805e902-955c-4443-a6b4-64e050b2e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572563d3-9fb2-4dc5-99c2-ff3cd56bd28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718c7b60-6062-45d0-9fde-18cf474d5cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='deprecated', *, C=1.0, l1_ratio=0.0, dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, verbose=0, warm_start=False, n_jobs=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  This class implements regularized logistic regression using a set of available\n",
      " |  solvers. **Note that regularization is applied by default**. It can handle both\n",
      " |  dense and sparse input `X`. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted (and\n",
      " |  copied).\n",
      " |  \n",
      " |  The solvers 'lbfgs', 'newton-cg', 'newton-cholesky' and 'sag' support only L2\n",
      " |  regularization with primal formulation, or no regularization. The 'liblinear'\n",
      " |  solver supports both L1 and L2 regularization (but not both, i.e. elastic-net),\n",
      " |  with a dual formulation only for the L2 penalty. The Elastic-Net (combination of L1\n",
      " |  and L2) regularization is only supported by the 'saga' solver.\n",
      " |  \n",
      " |  For :term:`multiclass` problems (whenever `n_classes >= 3`), all solvers except\n",
      " |  'liblinear' optimize the (penalized) multinomial loss. 'liblinear' only handles\n",
      " |  binary classification but can be extended to handle multiclass by using\n",
      " |  :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      " |      Specify the norm of the penalty:\n",
      " |  \n",
      " |      - `None`: no penalty is added;\n",
      " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      " |      - `'l1'`: add a L1 penalty term;\n",
      " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Some penalties may not work with some solvers. See the parameter\n",
      " |         `solver` below, to know the compatibility between the penalty and\n",
      " |         solver.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |      .. deprecated:: 1.8\n",
      " |         `penalty` was deprecated in version 1.8 and will be removed in 1.10.\n",
      " |         Use `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for\n",
      " |         `penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for\n",
      " |         `'penalty='elasticnet'`.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization. `C=np.inf` results in unpenalized logistic regression.\n",
      " |      For a visual example on the effect of tuning the `C` parameter\n",
      " |      with an L1 penalty, see:\n",
      " |      :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.0\n",
      " |      The Elastic-Net mixing parameter, with `0 <= l1_ratio <= 1`. Setting\n",
      " |      `l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.\n",
      " |      Any value between 0 and 1 gives an Elastic-Net penalty of the form\n",
      " |      `l1_ratio * L1 + (1 - l1_ratio) * L2`.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         Certain values of `l1_ratio`, i.e. some penalties, may not work with some\n",
      " |         solvers. See the parameter `solver` below, to know the compatibility between\n",
      " |         the penalty and solver.\n",
      " |  \n",
      " |      .. versionchanged:: 1.8\n",
      " |          Default value changed from None to 0.0.\n",
      " |  \n",
      " |      .. deprecated:: 1.8\n",
      " |          `None` is deprecated and will be removed in version 1.10. Always use\n",
      " |          `l1_ratio` to specify the penalty type.\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual (constrained) or primal (regularized, see also\n",
      " |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      " |      is only implemented for l2 penalty with liblinear solver. Prefer `dual=False`\n",
      " |      when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver `liblinear` is used\n",
      " |      and `self.fit_intercept` is set to `True`. In this case, `x` becomes\n",
      " |      `[x, self.intercept_scaling]`,\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      `intercept_scaling` is appended to the instance vector.\n",
      " |      The intercept becomes\n",
      " |      ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      .. note::\n",
      " |          The synthetic feature weight is subject to L1 or L2\n",
      " |          regularization as all other features.\n",
      " |          To lessen the effect of regularization on synthetic feature weight\n",
      " |          (and therefore on the intercept) `intercept_scaling` has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      " |      To choose a solver, you might want to consider the following aspects:\n",
      " |  \n",
      " |      - 'lbfgs' is a good default solver because it works reasonably well for a wide\n",
      " |        class of problems.\n",
      " |      - For :term:`multiclass` problems (`n_classes >= 3`), all solvers except\n",
      " |        'liblinear' minimize the full multinomial loss, 'liblinear' will raise an\n",
      " |        error.\n",
      " |      - 'newton-cholesky' is a good choice for\n",
      " |        `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n",
      " |        categorical features with rare categories. Be aware that the memory usage\n",
      " |        of this solver has a quadratic dependency on `n_features * n_classes`\n",
      " |        because it explicitly computes the full Hessian matrix.\n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      " |        and 'saga' are faster for large ones;\n",
      " |      - 'liblinear' can only handle binary classification by default. To apply a\n",
      " |        one-versus-rest scheme for the multiclass setting one can wrap it with the\n",
      " |        :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
      " |  \n",
      " |      .. warning::\n",
      " |         The choice of the algorithm depends on the penalty chosen (`l1_ratio=0`\n",
      " |         for L2-penalty, `l1_ratio=1` for L1-penalty and `0 < l1_ratio < 1` for\n",
      " |         Elastic-Net) and on (multinomial) multiclass support:\n",
      " |  \n",
      " |         ================= ======================== ======================\n",
      " |         solver            l1_ratio                 multinomial multiclass\n",
      " |         ================= ======================== ======================\n",
      " |         'lbfgs'           l1_ratio=0               yes\n",
      " |         'liblinear'       l1_ratio=1 or l1_ratio=0 no\n",
      " |         'newton-cg'       l1_ratio=0               yes\n",
      " |         'newton-cholesky' l1_ratio=0               yes\n",
      " |         'sag'             l1_ratio=0               yes\n",
      " |         'saga'            0<=l1_ratio<=1           yes\n",
      " |         ================= ======================== ======================\n",
      " |  \n",
      " |      .. note::\n",
      " |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      " |         with approximately the same scale. You can preprocess the data with\n",
      " |         a scaler from :mod:`sklearn.preprocessing`.\n",
      " |  \n",
      " |      .. seealso::\n",
      " |         Refer to the :ref:`User Guide <Logistic_regression>` for more\n",
      " |         information regarding :class:`LogisticRegression` and more specifically the\n",
      " |         :ref:`Table <logistic_regression_solvers>`\n",
      " |         summarizing solver/penalty supports.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient (SAG) descent solver. Multinomial support in\n",
      " |         version 0.18.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |      .. versionadded:: 1.2\n",
      " |         newton-cholesky solver. Multinomial support in version 1.6.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Does not have any effect.\n",
      " |  \n",
      " |      .. deprecated:: 1.8\n",
      " |         `n_jobs` is deprecated in version 1.8 and will be removed in 1.10.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (1, )\n",
      " |      Actual number of iterations for all classes.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log_loss\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      " |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.82e-01, 1.82e-02, 1.44e-08],\n",
      " |         [9.72e-01, 2.82e-02, 3.02e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97\n",
      " |  \n",
      " |  For a comparison of the LogisticRegression with other classifiers see:\n",
      " |  :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      " |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='deprecated', *, C=1.0, l1_ratio=0.0, dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, verbose=0, warm_start=False, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __sklearn_tags__(self)\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multiclass / multinomial problem the softmax function is used to find\n",
      " |      the predicted probability of each class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression\n",
      " |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant when this estimator is used as a\n",
      " |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      " |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression\n",
      " |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant when this estimator is used as a\n",
      " |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      " |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "help(lm.LogisticRegression)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f01fe3a-8545-4e10-9a16-455a4c5a4d21",
   "metadata": {},
   "source": [
    "dual: bool, default=False\n",
    "\n",
    "Dual (constrained) or primal (regularized, see also this equation) formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f035c623-f535-4e28-9a22-5b9c8d493409",
   "metadata": {},
   "source": [
    "fit_intercept: bool, default=True\n",
    "\n",
    "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e115b9c5-b2ef-41fc-b422-160649e7ed6a",
   "metadata": {},
   "source": [
    "class_weight : dict or 'balanced', default=None\n",
    "\n",
    "Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "\n",
    "The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as:\n",
    "\n",
    "n_samples / (n_classes * np.bincount(y)).\n",
    "\n",
    "Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n",
    "\n",
    "Added in version 0.17: class_weight='balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0d84a-6d72-487f-946f-87724a1f4b7f",
   "metadata": {},
   "source": [
    "##### solver {‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default='lbfgs'\n",
    "\n",
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "##### 'lbfgs'\n",
    "Limited-memory BFGS optimizer.  \n",
    "Good default choice for most problems.  \n",
    "Supports multinomial loss.\n",
    "\n",
    "##### 'liblinear'\n",
    "Uses coordinate descent.  \n",
    "Good for small datasets.  \n",
    "Supports L1 and L2 penalties.  \n",
    "Does **not** support multinomial loss.\n",
    "\n",
    "##### 'newton-cg'\n",
    "Newton’s method using conjugate gradient.  \n",
    "Supports multinomial loss.\n",
    "\n",
    "##### 'newton-cholesky'\n",
    "Newton’s method using Cholesky decomposition.  \n",
    "Efficient for datasets with large number of samples.\n",
    "\n",
    "##### 'sag'\n",
    "Stochastic Average Gradient.  \n",
    "Faster for large datasets.\n",
    "\n",
    "##### 'saga'\n",
    "Variant of SAG.  \n",
    "Supports L1, L2, and ElasticNet penalties.  \n",
    "Works well for large datasets and sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f8fea-ba55-4424-9f06-97abd39814b8",
   "metadata": {},
   "source": [
    "## [Documentation](!https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98121f7b-e593-43ee-86e2-11726ef1f6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
